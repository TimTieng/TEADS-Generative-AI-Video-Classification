{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech Exellence Advanced Data Science - Generative AI Video Classification Project\n",
    "\n",
    "##### Project Authors: Tim Tieng, Afia Owusu-Forfie\n",
    "\n",
    "**Objective**: Develop a model to classify video content into categories such as sports, news, movies, etc., and enhance this classification by generating descriptive captions or summaries that provide additional context about the content. This can be particularly useful for content curation platforms, accessibility applications (e.g., providing descriptions for the hearing impaired), or educational tools where supplementary information enhances learning.\n",
    "\n",
    "**Data**: Public Dataset: Use a dataset like the YouTube-8M, which has a vast collection of labeled video data suitable for training video classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages for project\n",
    "\n",
    "# Standard Libraries\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Algorithms, Modeling and Data Pre-processing\n",
    "import feature_engine\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from scipy.stats import anderson, chi2_contingency\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,precision_score, roc_auc_score,recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Deep Learning\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import RandomFlip, RandomRotation, Rescaling, BatchNormalization, Conv2D, MaxPooling2D, Dense, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Model Optimization and Hyperparameter Tuning\n",
    "import hyperopt\n",
    "from hyperopt import STATUS_OK, Trials, fmin, tpe, hp\n",
    "import mlflow\n",
    "\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the data - Due to issues with larger dataset, the project group is using the dataset provided via kaggle competition that used youtube-8M data\n",
    "\n",
    "frame_level_record1 = \"../data/frame-sample/frame/train00.tfrecord\"\n",
    "frame_level_record2 = \"../data/frame-sample/frame/train01.tfrecord\"\n",
    "validation_level_record1 = \"../data/validate-sample/validate/validate00.tfrecord\"\n",
    "validation_level_record2 = \"../data/validate-sample/validate/validate01.tfrecord\"\n",
    "print(f\"Frame Directory Data Present: {os.listdir('../data/frame-sample/frame')}\")\n",
    "print(f\"Validation Directory Data Present: {os.listdir('../data/validate-sample/validate')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that we have frame-level data and validation data loaded into our project directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Video-Level Information from the frame-level files: train00.tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tim Test/Experimental code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "    \"\"\"\n",
    "    Parses a single example from a TFRecord file into a tensor suitable for training or evaluation.\n",
    "\n",
    "    This function defines and uses a fixed schema to parse each example in the TFRecord file. The schema is defined using\n",
    "    TensorFlow's parsing functions which map the data from a serialized `tf.train.Example` protobuf to tensors. The keys\n",
    "    in the `feature_description` dictionary specify the expected features in the TFRecord, and their corresponding values\n",
    "    define the type and shape of the data.\n",
    "\n",
    "    Parameters:\n",
    "    example_proto (tf.Tensor): A tensor containing a serialized `tf.train.Example` protobuf.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where each key corresponds to a feature specified in the TFRecord schema. Each value is a\n",
    "    TensorFlow tensor. The keys and their respective tensors are:\n",
    "        - 'id': A tensor containing the unique identifier of the video. This is a scalar string tensor.\n",
    "        - 'labels': A sparse tensor containing a list of integer labels associated with the video.\n",
    "        - 'rgb': A dense tensor of shape [1024] containing the RGB features of the video frame.\n",
    "        - 'audio': A dense tensor of shape [128] containing the audio features of the video frame.\n",
    "        - 'segment_start_times': (Optional) A sparse tensor containing start times for each labeled segment.\n",
    "        - 'segment_end_times': (Optional) A sparse tensor containing end times for each labeled segment.\n",
    "        - 'segment_labels': (Optional) A sparse tensor containing labels for each segment.\n",
    "        - 'segment_scores': (Optional) A sparse tensor containing binary scores indicating positive or negative sentiment for each segment label.\n",
    "\n",
    "    The optional keys ('segment_start_times', 'segment_end_times', 'segment_labels', 'segment_scores') should be uncommented\n",
    "    in the feature description if segment-level data is being processed.\n",
    "\n",
    "    Example:\n",
    "    To use this function, ensure it is mapped over a dataset created from a TFRecord file, like so:\n",
    "    dataset = tf.data.TFRecordDataset(\"path_to_tfrecord_file.tfrecord\")\n",
    "    parsed_dataset = dataset.map(parse_tfrecord)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define your feature description\n",
    "    feature_description = {\n",
    "        'id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'labels': tf.io.VarLenFeature(tf.int64),\n",
    "        'rgb': tf.io.FixedLenFeature([1024], tf.float32, default_value=np.zeros([1024], dtype=np.float32)),\n",
    "        'audio': tf.io.FixedLenFeature([128], tf.float32, default_value=np.zeros([128], dtype=np.float32)),\n",
    "        # Uncomment these if you're handling segment data\n",
    "        'segment_start_times': tf.io.VarLenFeature(tf.int64),\n",
    "        'segment_end_times': tf.io.VarLenFeature(tf.int64),\n",
    "        'segment_labels': tf.io.VarLenFeature(tf.int64),\n",
    "        'segment_scores': tf.io.VarLenFeature(tf.float32),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Purpose: To take load tfrecord files for future manipulation\n",
    "    Arguments: a filepath or variable that stores a filepath to a tfrecord file\n",
    "    \"\"\"\n",
    "    raw_dataset = tf.data.TFRecordDataset(file_path)\n",
    "    parsed_dataset = raw_dataset.map(parse_tfrecord) # map the data\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset object using the load_dataset() which calls in parse_tfrecord()\n",
    "dataset = load_dataset(frame_level_record1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View and inspect parsed dataset of train00.tfrecord\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are over observations, printing only the first 20\n",
    "for index, item in enumerate(dataset.as_numpy_iterator()):\n",
    "    if index < 10:\n",
    "        print(item)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "We can see a each record can have numerous label values. The values in the label list of each item can be mapped to the values in the vocabulary data dictionary. \n",
    "\n",
    "**Audio** - this is an empty list that was pre-populated with np.zeroes during the collection phase\n",
    "\n",
    "**RGB** - like audio, this is an array or np.zeros\n",
    "\n",
    "**Segment_X** thes are all empty arrays. these features are labeled as optional as per youtube-8m documenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary from the 'vocabulary.csv' file\n",
    "vocabulary = {}\n",
    "with open('../data/vocabulary.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        numerical_value = row[0]  # Assuming numerical values are in the first column\n",
    "        name = row[3]                   # Assuming names are in the second column\n",
    "        vocabulary[numerical_value] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm that the index value and name are created into the dictionary\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Vocabulary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the vocabulary csv file\n",
    "vocab_path = \"../data/vocabulary.csv\"\n",
    "vocab= pd.read_csv(vocab_path)\n",
    "vocab_df = pd.DataFrame(vocab)\n",
    "\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value count and percentage per column\n",
    "vocab_df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value count and percentage per column\n",
    "vocab_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique labels in the vocabulary df\n",
    "unique_labels = vocab_df['Name'].unique()\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label_count = vocab_df['Name'].nunique()\n",
    "print(f\"Unique Video Labels: {unique_label_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 50 videl labels present \n",
    "\n",
    "n = 50 # although, we'll only show those that appear in the 1,000 for this competition\n",
    "top_n = Counter([item for sublist in labels for item in sublist]).most_common(n)\n",
    "top_n_labels = [int(i[0]) for i in top_n]\n",
    "top_n_label_names = [label_mapping[x] for x in top_n_labels if x in label_mapping] # filter out the labels that aren't in the 1,000 used for this competition\n",
    "print(top_n_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the frequencie's labels\n",
    "\n",
    "labels_count_dict = dict(top_n)\n",
    "labels_count_df = pd.DataFrame.from_dict(labels_count_dict, orient='index').reset_index()\n",
    "labels_count_df.columns = ['label', 'count']\n",
    "labels_count_df['label'] = labels_count_df['label'].map(label_mapping, na_action='ignore')\n",
    "TOP_labels = list(labels_count_df['label'])[:n]\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.barplot(y='label', x='count', data=labels_count_df)\n",
    "plt.title('Top {} labels with sample count'.format(n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TechEx_Project_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
