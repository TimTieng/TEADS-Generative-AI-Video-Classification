{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech Exellence Advanced Data Science - Generative AI Video Classification Project\n",
    "\n",
    "##### Project Authors: Tim Tieng, Afia Owusu-Forfie\n",
    "\n",
    "**Objective**: Develop a model to classify video content into categories such as sports, news, movies, etc., and enhance this classification by generating descriptive captions or summaries that provide additional context about the content. This can be particularly useful for content curation platforms, accessibility applications (e.g., providing descriptions for the hearing impaired), or educational tools where supplementary information enhances learning.\n",
    "\n",
    "**Data**: Public Dataset: Use a dataset like the YouTube-8M, which has a vast collection of labeled video data suitable for training video classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install chardet\n",
    "# %pip install utils\n",
    "# %pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages for project\n",
    "\n",
    "# Standard Libraries\n",
    "import csv\n",
    "from IPython.display import YouTubeVideo\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import utils\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Algorithms, Modeling and Data Pre-processing\n",
    "import feature_engine\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from scipy.stats import anderson, chi2_contingency\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,precision_score, roc_auc_score,recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Deep Learning\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import RandomFlip, RandomRotation, Rescaling, BatchNormalization, Conv2D, MaxPooling2D, Dense, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Model Optimization and Hyperparameter Tuning\n",
    "import hyperopt\n",
    "from hyperopt import STATUS_OK, Trials, fmin, tpe, hp\n",
    "import mlflow\n",
    "\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the data - Due to issues with larger dataset, the project group is using the dataset provided via kaggle competition that used youtube-8M data\n",
    "# Create variables that hold the filepath destination to train and validate TFrecord files\n",
    "frame_level_record1 = \"../data/frame-sample/frame/train00.tfrecord\"\n",
    "frame_level_record2 = \"../data/frame-sample/frame/train01.tfrecord\"\n",
    "validation_level_record1 = \"../data/validate-sample/validate/validate00.tfrecord\"\n",
    "validation_level_record2 = \"../data/validate-sample/validate/validate01.tfrecord\"\n",
    "\n",
    "# new data\n",
    "frame_train1 = \"../data/train0255.tfrecord\"\n",
    "\n",
    "print(f\"Frame Directory TFrecords Present: {os.listdir('../data/frame-sample/frame')}\")\n",
    "print(f\"Validation Directory TFrecords Present: {os.listdir('../data/validate-sample/validate')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in TFRecord files IAW Tensorflow documentation - https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "filenames = [frame_train1]\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_record in raw_dataset.take(5):\n",
    "    print(repr(raw_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the records in the raw_dataset object to view its original contents\n",
    "for raw_record in raw_dataset.take(5):\n",
    "  example = tf.train.Example()\n",
    "  example.ParseFromString(raw_record.numpy())\n",
    "  print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "# example.features.feature is the dictionary\n",
    "for key, feature in example.features.feature.items():\n",
    "  # The values are the Feature objects which contain a `kind` which contains:\n",
    "  # one of three fields: bytes_list, float_list, int64_list\n",
    "\n",
    "  kind = feature.WhichOneof('kind')\n",
    "  result[key] = np.array(getattr(feature, kind).value)\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames2 = [frame_train1, frame_train2,frame_train3]\n",
    "raw_dataset2 = tf.data.TFRecordDataset(filenames2)\n",
    "raw_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_record in raw_dataset2.take(5):\n",
    "  example2 = tf.train.Example()\n",
    "  example2.ParseFromString(raw_record.numpy())\n",
    "  print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = {}\n",
    "# example.features.feature is the dictionary\n",
    "for key, feature in example2.features.feature.items():\n",
    "  # The values are the Feature objects which contain a `kind` which contains:\n",
    "  # one of three fields: bytes_list, float_list, int64_list\n",
    "\n",
    "  kind = feature.WhichOneof('kind')\n",
    "  result2[key] = np.array(getattr(feature, kind).value)\n",
    "\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations - Raw TFrecord files\n",
    "\n",
    "After printing raw records, we can see that there seems to be only 2 attributes in train00 and train01 tfrecord files: labels and id (video). As of now, this suggest that the data is not detailed enough for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection & Preprocessing: Tim's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "    \"\"\"\n",
    "    Parses a single example from a TFRecord file into a tensor suitable for training or evaluation.\n",
    "\n",
    "    This function defines and uses a fixed schema to parse each example in the TFRecord file. The schema is defined using\n",
    "    TensorFlow's parsing functions which map the data from a serialized `tf.train.Example` protobuf to tensors. The keys\n",
    "    in the `feature_description` dictionary specify the expected features in the TFRecord, and their corresponding values\n",
    "    define the type and shape of the data.\n",
    "\n",
    "    Parameters:\n",
    "    example_proto (tf.Tensor): A tensor containing a serialized `tf.train.Example` protobuf.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where each key corresponds to a feature specified in the TFRecord schema. Each value is a\n",
    "    TensorFlow tensor. The keys and their respective tensors are:\n",
    "        - 'id': A tensor containing the unique identifier of the video. This is a scalar string tensor.\n",
    "        - 'labels': A sparse tensor containing a list of integer labels associated with the video.\n",
    "        - 'rgb': A dense tensor of shape [1024] containing the RGB features of the video frame.\n",
    "        - 'audio': A dense tensor of shape [128] containing the audio features of the video frame.\n",
    "        - 'segment_start_times': (Optional) A sparse tensor containing start times for each labeled segment.\n",
    "        - 'segment_end_times': (Optional) A sparse tensor containing end times for each labeled segment.\n",
    "        - 'segment_labels': (Optional) A sparse tensor containing labels for each segment.\n",
    "        - 'segment_scores': (Optional) A sparse tensor containing binary scores indicating positive or negative sentiment for each segment label.\n",
    "\n",
    "    The optional keys ('segment_start_times', 'segment_end_times', 'segment_labels', 'segment_scores') should be uncommented\n",
    "    in the feature description if segment-level data is being processed.\n",
    "\n",
    "    Example:\n",
    "    To use this function, ensure it is mapped over a dataset created from a TFRecord file, like so:\n",
    "    dataset = tf.data.TFRecordDataset(\"path_to_tfrecord_file.tfrecord\")\n",
    "    parsed_dataset = dataset.map(parse_tfrecord)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define your feature description\n",
    "    feature_description = {\n",
    "        'id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'labels': tf.io.VarLenFeature(tf.int64),\n",
    "        'rgb': tf.io.FixedLenSequenceFeature([], dtype=tf.string, allow_missing=True),\n",
    "        'audio': tf.io.FixedLenSequenceFeature([], dtype=tf.string, allow_missing=True),\n",
    "        #'rgb': tf.io.FixedLenFeature([1024], tf.float32, default_value=np.zeros([1024], dtype=np.float32)),\n",
    "        #'audio': tf.io.FixedLenFeature([128], tf.float32, default_value=np.zeros([128], dtype=np.float32)),\n",
    "        # Uncomment these if you're handling segment data\n",
    "        'segment_start_times': tf.io.VarLenFeature(tf.int64),\n",
    "        'segment_end_times': tf.io.VarLenFeature(tf.int64),\n",
    "        'segment_labels': tf.io.VarLenFeature(tf.int64),\n",
    "        'segment_scores': tf.io.VarLenFeature(tf.float32),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Purpose: To take load tfrecord files for future manipulation\n",
    "    Arguments: a filepath or variable that stores a filepath to a tfrecord file\n",
    "    \"\"\"\n",
    "    raw_dataset = tf.data.TFRecordDataset(file_path)\n",
    "    parsed_dataset = raw_dataset.map(parse_tfrecord) # map the data\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset object using the load_dataset() which calls in parse_tfrecord()\n",
    "parsed_dataset = load_dataset(frame_level_record1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View and inspect parsed dataset of train00.tfrecord\n",
    "parsed_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(parsed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parsed_record in parsed_dataset.take(10):\n",
    "  print(repr(parsed_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "We can see a each record can have numerous label values. The values in the label list of each item can be mapped to the values in the vocabulary data dictionary. \n",
    "\n",
    "**Audio** - this is an empty list that was pre-populated with np.zeroes during the collection phase\n",
    "\n",
    "**RGB** - like audio, this is an array or np.zeros\n",
    "\n",
    "**Segment_X** thes are all empty arrays. these features are labeled as optional as per youtube-8m documenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset2 = load_dataset(frame_train1)\n",
    "parsed_dataset2.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parsed_record in parsed_dataset2.take(20):\n",
    "  print(repr(parsed_record['audio']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parsed_record in parsed_dataset2.take(-10):\n",
    "  print(repr(parsed_record['rgb']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_ids = []\n",
    "labels = []\n",
    "\n",
    "for parsed_record in parsed_dataset:\n",
    "    vid_ids.append(parsed_record['id'].numpy().decode('UTF-8'))\n",
    "    # For labels, which are parsed as VarLenFeature, convert them to a dense tensor first\n",
    "    labels.append(tf.sparse.to_dense(parsed_record['labels']).numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of videos in this tfrecord: ',len(vid_ids))\n",
    "print('Number of labels in this tfrecord: ',len(labels))\n",
    "print('Picking a youtube video id:',vid_ids[455])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"List of Youtube Video ID's - Anonymized\\n\\n{vid_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "The output above provides anonyized video ids as per privacy concerns provided by Youtube. We can pass in these values to this website, to get the real youtube video id value: http://data.yt8m.org/2/j/i/<FirstTwoChars>/<CompleteAnonymizedID>.js . This will open up a web browser window with a dictionary value present. You will use this website to get the full id.\n",
    "\n",
    "**Example** http://data.yt8m.org/2/j/i/0V/0V00.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('guQVEOzjIU8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_frames(sequence_parsed):\n",
    "    \"\"\"\n",
    "    Purpose: Decode and process frame data for RGB and audio from a parsed TFRecord SequenceExample.\n",
    "\n",
    "    This function takes a parsed SequenceExample which includes features for 'rgb' and 'audio'. \n",
    "    Each of these features is expected to be a sequence of byte strings, where each byte string \n",
    "    encodes a frame of RGB or audio data. The function decodes these byte strings into uint8 tensors,\n",
    "    casts them to float32 for further processing, and optionally normalizes them if required.\n",
    "\n",
    "    Parameters:\n",
    "    sequence_parsed (dict of tf.Tensor): A dictionary containing at least two keys, 'rgb' and 'audio', \n",
    "    each associated with a tensor of raw byte strings. Each tensor represents a sequence of frames\n",
    "    encoded as byte strings.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple of two tf.Tensor items:\n",
    "        - The first tensor contains the RGB frames as a tf.float32 tensor.\n",
    "        - The second tensor contains the audio frames as a tf.float32 tensor.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    # Assuming `dataset` is a tf.data.Dataset object containing parsed SequenceExample records\n",
    "    for sequence_parsed in dataset:\n",
    "        rgb_frames, audio_frames = decode_frames(sequence_parsed)\n",
    "        # Now `rgb_frames` and `audio_frames` can be used for further processing, such as model input\n",
    "    \"\"\"\n",
    "\n",
    "    rgb_frames = tf.map_fn(lambda x: tf.io.decode_raw(x, tf.uint8), sequence_parsed['rgb'], dtype=tf.uint8)\n",
    "    audio_frames = tf.map_fn(lambda x: tf.io.decode_raw(x, tf.uint8), sequence_parsed['audio'], dtype=tf.uint8)\n",
    "    # Convert from uint8 to float32 and normalize if required\n",
    "    rgb_frames = tf.cast(rgb_frames, tf.float32)\n",
    "    audio_frames = tf.cast(audio_frames, tf.float32)\n",
    "    return rgb_frames, audio_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_rgb = []\n",
    "feat_audio = []\n",
    "\n",
    "# Iterate through the dataset\n",
    "for sequence_parsed in parsed_dataset2:\n",
    "    rgb_frames, audio_frames = decode_frames(sequence_parsed)\n",
    "    # Collect decoded frames\n",
    "    feat_rgb.append(rgb_frames.numpy())  # Convert to numpy array\n",
    "    feat_audio.append(audio_frames.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first video has %d frames' %len(feat_rgb[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "This confirms the anomaly that rgb and audio values are not present in the tfrecords dataset. \n",
    "\n",
    "**Evidence 1** - had to either set the 'allow_missing()' argument to True in the FixedLengthSequence() functions for both RGB and Audio attributes.\n",
    "\n",
    "**Evidence 2** - Previously set the default values to np.zero matrix for those two variables; printed out 0's for both attributes.\n",
    "\n",
    "**Evidence 3** - Output above shows that the first element in tfrecord has 0 frames.\n",
    "\n",
    "**Evidence 4** - Ouotput of raw_dataset shows only labels and id's having values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary from the 'vocabulary.csv' file\n",
    "vocabulary = {}\n",
    "with open('../data/vocabulary.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        numerical_value = row[0]  # index of \"index\" in vocab csv\n",
    "        name = row[3]   # Index of 'name' column\n",
    "        vocabulary[numerical_value] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm that the index value and name are created into the dictionary\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Map labels to the values in label array in parsed dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Vocabulary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the vocabulary csv file\n",
    "vocab_path = \"../data/vocabulary.csv\"\n",
    "vocab= pd.read_csv(vocab_path)\n",
    "vocab_df = pd.DataFrame(vocab)\n",
    "\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value count and percentage per column\n",
    "vocab_df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value count and percentage per column\n",
    "vocab_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique labels in the vocabulary df\n",
    "unique_labels = vocab_df['Name'].unique()\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label_count = vocab_df['Name'].nunique()\n",
    "print(f\"Unique Video Labels: {unique_label_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50 # although, we'll only show those that appear in the 1,000 for this competition\n",
    "from collections import Counter\n",
    "\n",
    "label_mapping =  vocab[['Index', 'Name']].set_index('Index', drop=True).to_dict()['Name']\n",
    "\n",
    "top_n = Counter([item for sublist in labels for item in sublist]).most_common(n)\n",
    "top_n_labels = [int(i[0]) for i in top_n]\n",
    "top_n_label_names = [label_mapping[x] for x in top_n_labels if x in label_mapping] # filter out the labels that aren't in the 1,000 used for this competition\n",
    "print(top_n_label_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the frequencie's labels\n",
    "\n",
    "labels_count_dict = dict(top_n)\n",
    "labels_count_df = pd.DataFrame.from_dict(labels_count_dict, orient='index').reset_index()\n",
    "labels_count_df.columns = ['label', 'count']\n",
    "labels_count_df['label'] = labels_count_df['label'].map(label_mapping, na_action='ignore')\n",
    "TOP_labels = list(labels_count_df['label'])[:n]\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.barplot(y='label', x='count', data=labels_count_df)\n",
    "plt.title('Top {} labels with sample count'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vert1_count = vocab_df['Vertical1'].nunique()\n",
    "print(f\"Unique Vertical1 Values: {unique_vert1_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TechEx_Project_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
