{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7idd-13b8OW"
      },
      "source": [
        "### Technology Excellence - Advanced Data Science (TEADS) - Generative AI Video Classification Project\n",
        "\n",
        "#####  **Project Authors:** Tim Tieng and Afia Owusu-Forfie\n",
        "\n",
        "##### **Dates:** April 22, 2024 to April 26, 2024\n",
        "\n",
        "##### **Team Name:** Team 5 - AI Now\n",
        "\n",
        "##### **Project Module:** Data Collection and Preparation\n",
        "\n",
        "**Objective**: Develop a model to classify video content into categories such as sports, news, movies, etc., and enhance this classification by generating descriptive captions or summaries that provide additional context about the content. This can be particularly useful for content curation platforms, accessibility applications (e.g., providing descriptions for the hearing impaired), or educational tools where supplementary information enhances learning.\n",
        "\n",
        "**Data**: Public Dataset: Use a dataset like the YouTube-8M, which has a vast collection of labeled video data suitable for training video classification models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyGbVgVIb8OY",
        "outputId": "515735e3-2336-4c1f-d504-38b1bf4470c5"
      },
      "outputs": [],
      "source": [
        "!pip install chardet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxKJ2RwTe2Pt",
        "outputId": "6b407a46-ed4b-494a-81aa-ed787aab5f7b"
      },
      "outputs": [],
      "source": [
        "!pip install feature_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Equ_LSRfT8O"
      },
      "outputs": [],
      "source": [
        "#!pip install -r requirements.txt --use-deprecated=legacy-resolver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKhyJW5Rb8OY"
      },
      "outputs": [],
      "source": [
        "# Import Packages for project\n",
        "\n",
        "# Standard Libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Algorithms, Modeling and Data Pre-processing\n",
        "\n",
        "import feature_engine\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine.transformation import YeoJohnsonTransformer\n",
        "from scipy.stats import anderson, chi2_contingency\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,precision_score, roc_auc_score,recall_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Deep Learning\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import RandomFlip, RandomRotation, Rescaling, BatchNormalization, Conv2D, MaxPooling2D, Dense, Input\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam, SGD\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Model Optimization and Hyperparameter Tuning\n",
        "import hyperopt\n",
        "from hyperopt import STATUS_OK, Trials, fmin, tpe, hp\n",
        "#import mlflow\n",
        "\n",
        "import tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt0hMy6qhnGH",
        "outputId": "d5e87e1f-db38-4d62-b75d-c0ae67cd80ef"
      },
      "outputs": [],
      "source": [
        "# unzipping the files in the frame-sample zip folder\n",
        "!unzip \"/content/frame-sample.zip\" -d '/content/frame-sample'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHgq9TepCWFs"
      },
      "source": [
        "For some reason the same !unzip command does not work for frame-sample.zip eventhough it works for validate-sample.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZD1ibGoiKZj",
        "outputId": "97a5851c-3bcd-427c-c39b-1ee7b7087b01"
      },
      "outputs": [],
      "source": [
        "# unzipping the files in the validate-sample zip folder\n",
        "!unzip '/content/validate-sample.zip' -d '/content/validate-sample'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Jh01UXb8OZ",
        "outputId": "0befa4d7-060a-40fe-9a8e-752624276c86"
      },
      "outputs": [],
      "source": [
        "# Obtain the data - Due to issues with larger dataset, the project group is using the dataset provided via kaggle competition that used youtube-8M data\n",
        "\n",
        "frame_level_record1 = \"/content/frame-sample/frame/train00.tfrecord\"\n",
        "frame_level_record2 = \"/content/frame-sample/frame/train01.tfrecord\"\n",
        "validation_level_record1 = \"/content/validate-sample/validate/validate00.tfrecord\"\n",
        "validation_level_record2 = \"/content/validate-sample/validate/validate01.tfrecord\"\n",
        "print(f\"Frame Directory Data Present: {os.listdir('/content/frame-sample/frame')}\")\n",
        "print(f\"Validation Directory Data Present: {os.listdir('/content/validate-sample/validate')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcULBLSwb8Oa"
      },
      "source": [
        "This confirms that we have frame-level data and validation data loaded into our project directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8syY591vb8Oa"
      },
      "source": [
        "Extract Video-Level Information from the frame-level files: train00.tfrecord"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONMPvuuDb8Oa"
      },
      "source": [
        "### Tim Test/Experimental code begins here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPKp6Qfub8Oa"
      },
      "outputs": [],
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "    \"\"\"\n",
        "    Parses a single example from a TFRecord file into a tensor suitable for training or evaluation.\n",
        "\n",
        "    This function defines and uses a fixed schema to parse each example in the TFRecord file. The schema is defined using\n",
        "    TensorFlow's parsing functions which map the data from a serialized `tf.train.Example` protobuf to tensors. The keys\n",
        "    in the `feature_description` dictionary specify the expected features in the TFRecord, and their corresponding values\n",
        "    define the type and shape of the data.\n",
        "\n",
        "    Parameters:\n",
        "    example_proto (tf.Tensor): A tensor containing a serialized `tf.train.Example` protobuf.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary where each key corresponds to a feature specified in the TFRecord schema. Each value is a\n",
        "    TensorFlow tensor. The keys and their respective tensors are:\n",
        "        - 'id': A tensor containing the unique identifier of the video. This is a scalar string tensor.\n",
        "        - 'labels': A sparse tensor containing a list of integer labels associated with the video.\n",
        "        - 'rgb': A dense tensor of shape [1024] containing the RGB features of the video frame.\n",
        "        - 'audio': A dense tensor of shape [128] containing the audio features of the video frame.\n",
        "        - 'segment_start_times': (Optional) A sparse tensor containing start times for each labeled segment.\n",
        "        - 'segment_end_times': (Optional) A sparse tensor containing end times for each labeled segment.\n",
        "        - 'segment_labels': (Optional) A sparse tensor containing labels for each segment.\n",
        "        - 'segment_scores': (Optional) A sparse tensor containing binary scores indicating positive or negative sentiment for each segment label.\n",
        "\n",
        "    The optional keys ('segment_start_times', 'segment_end_times', 'segment_labels', 'segment_scores') should be uncommented\n",
        "    in the feature description if segment-level data is being processed.\n",
        "\n",
        "    Example:\n",
        "    To use this function, ensure it is mapped over a dataset created from a TFRecord file, like so:\n",
        "    dataset = tf.data.TFRecordDataset(\"path_to_tfrecord_file.tfrecord\")\n",
        "    parsed_dataset = dataset.map(parse_tfrecord)\n",
        "    \"\"\"\n",
        "\n",
        "    # Define your feature description\n",
        "    feature_description = {\n",
        "        'id': tf.io.FixedLenFeature([], tf.string),\n",
        "        'labels': tf.io.VarLenFeature(tf.int64),\n",
        "        'rgb': tf.io.FixedLenFeature([1024], tf.float32, default_value=np.zeros([1024], dtype=np.float32)),\n",
        "        'audio': tf.io.FixedLenFeature([128], tf.float32, default_value=np.zeros([128], dtype=np.float32)),\n",
        "        # Uncomment these if you're handling segment data\n",
        "        'segment_start_times': tf.io.VarLenFeature(tf.int64),\n",
        "        'segment_end_times': tf.io.VarLenFeature(tf.int64),\n",
        "        'segment_labels': tf.io.VarLenFeature(tf.int64),\n",
        "        'segment_scores': tf.io.VarLenFeature(tf.float32),\n",
        "    }\n",
        "    return tf.io.parse_single_example(example_proto, feature_description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKmHyEb3b8Oa"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Purpose: To take load tfrecord files for future manipulation\n",
        "    Arguments: a filepath or variable that stores a filepath to a tfrecord file\n",
        "    \"\"\"\n",
        "    raw_dataset = tf.data.TFRecordDataset(file_path)\n",
        "    parsed_dataset = raw_dataset.map(parse_tfrecord) # map the data\n",
        "    return parsed_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zsK6NK2b8Ob"
      },
      "outputs": [],
      "source": [
        "# Create a dataset object using the load_dataset() which calls in parse_tfrecord()\n",
        "dataset = load_dataset(frame_level_record1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVX-v6Beb8Ob"
      },
      "outputs": [],
      "source": [
        "#View and inspect parsed dataset of train00.tfrecord\n",
        "dataset.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtjhizZwb8Ob"
      },
      "source": [
        "descriptive stats on segments\n",
        "\n",
        "pd dataframe - only import\n",
        "\n",
        "Figure out how to access data of a tensorflow mapdata object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgZju71ib8Ob"
      },
      "outputs": [],
      "source": [
        "type(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNh0nqAGb8Ob"
      },
      "outputs": [],
      "source": [
        "# Since there are over observations, printing only the first 20\n",
        "for index, item in enumerate(dataset.as_numpy_iterator()):\n",
        "    if index < 10:\n",
        "        print(item)\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HNIsCzyb8Ob"
      },
      "source": [
        "### Observations\n",
        "\n",
        "We can see a each record can have numerous label values. The values in the label list of each item can be mapped to the values in the vocabulary data dictionary.\n",
        "\n",
        "**Audio** - this is an empty list that was pre-populated with np.zeroes during the collection phase\n",
        "\n",
        "**RGB** - like audio, this is an array or np.zeros\n",
        "\n",
        "**Segment_X** thes are all empty arrays. these features are labeled as optional as per youtube-8m documenation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Vc2IQa2lWOT"
      },
      "outputs": [],
      "source": [
        "#added by Afia\n",
        "!pip install pandas\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnr__eDxJOFL"
      },
      "source": [
        "This code did NOT work in the Colab environment for Afia eventhough it worked for Tim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dHupRpob8Oc"
      },
      "outputs": [],
      "source": [
        "# # Read in the vocabulary csv file\n",
        "# vocab_path = \"/content/vocabulary.csv\"\n",
        "# vocab= pd.read_csv(vocab_path)\n",
        "# vocab_df = pd.DataFrame(vocab)\n",
        "\n",
        "# vocab_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyHFAiKYb8Oc"
      },
      "outputs": [],
      "source": [
        "# unique_labels = vocab_df['Name'].unique()\n",
        "# unique_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w7P6_IMb8Oc"
      },
      "outputs": [],
      "source": [
        "# unique_label_count = vocab_df['Name'].nunique()\n",
        "# print(f\"Unique Video Labels: {unique_label_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iOnUbr4JiZa"
      },
      "source": [
        "## Tim Test/Experimental code ends here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-T0qcOkrMZB"
      },
      "outputs": [],
      "source": [
        "## Experimenting with information from Kaggle User: Kranti Kumar\n",
        "# https://www.kaggle.com/code/jagannathrk/analysis-youtube8m-2019"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9q9-ZyHrVPe"
      },
      "outputs": [],
      "source": [
        "frame_lvl_record = \"/content/frame-sample/frame/train00.tfrecord\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPFe0aSnb8Oc",
        "outputId": "f7a355fa-419d-457c-ba4a-b1a85f1e2d72"
      },
      "outputs": [],
      "source": [
        "print(os.listdir(\"/content/frame-sample/frame\"))\n",
        "print(os.listdir(\"/content/validate-sample/validate\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pzYVep_DOND",
        "outputId": "b787c385-973b-4d8f-a7fe-1014021caf25"
      },
      "outputs": [],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLEQl4pEDTyB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHcPG3ZeC15q",
        "outputId": "25bd518d-cb13-4795-e04f-aac1c8f013b8"
      },
      "outputs": [],
      "source": [
        "sub = pd.read_csv('/content/sample_submission.csv')\n",
        "sub.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XadzhwdeDjW8"
      },
      "outputs": [],
      "source": [
        "# sub.head() #did NOT work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7O8w2Z0r5kT"
      },
      "source": [
        "Exploring Data (TFRecord format) using a subsample of the YouTube-8M video & frame-level data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfNQBnqSukwY"
      },
      "outputs": [],
      "source": [
        "# from tensorflow import python_io #python_io broke the code. do NOT use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDd6QYVvvYdn",
        "outputId": "fa814144-324a-4411-e13e-98788361ee94"
      },
      "outputs": [],
      "source": [
        "!pip show tensorflow #added by Afia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW5I01G7KAaa"
      },
      "source": [
        "1st attempt did NOT work. Threw this error: AttributeError: module 'tensorflow' has no attribute 'python_io'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDRyIaQruZoY"
      },
      "outputs": [],
      "source": [
        "# vid_ids = []\n",
        "# labels = []\n",
        "\n",
        "# for example in tf.python_io.tf_record_iterator(frame_lvl_record):\n",
        "#     tf_example = tf.train.Example.FromString(example)\n",
        "#     vid_ids.append(tf_example.features.feature['id']\n",
        "#                    .bytes_list.value[0].decode(encoding='UTF-8'))\n",
        "#     labels.append(tf_example.features.feature['labels'].int64_list.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHP0cxYmKNtR"
      },
      "source": [
        "2nd attempt also did NOT work. Threw this error: AttributeError: module 'tensorflow._api.v2.io' has no attribute 'tf_record_iterator'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izVsWhJ-vjjk"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# vid_ids = []\n",
        "# labels = []\n",
        "\n",
        "# for example in tf.io.tf_record_iterator(frame_lvl_record):\n",
        "#     tf_example = tf.train.Example.FromString(example)\n",
        "#     vid_ids.append(tf_example.features.feature['id']\n",
        "#                    .bytes_list.value[0].decode(encoding='UTF-8'))\n",
        "#     labels.append(tf_example.features.feature['labels'].int64_list.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnIBuLmNKWr1"
      },
      "source": [
        "Using the suggested code by Colab, the third attempt worked!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L7lU3u38vztu",
        "outputId": "4a4e0b11-3a87-4a0a-f2e9-7d9af177f600"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.8\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "vid_ids = []\n",
        "labels = []\n",
        "\n",
        "for example in tf.compat.v1.io.tf_record_iterator(frame_lvl_record):\n",
        "    tf_example = tf.train.Example.FromString(example)\n",
        "    vid_ids.append(tf_example.features.feature['id']\n",
        "                   .bytes_list.value[0].decode(encoding='UTF-8'))\n",
        "    labels.append(tf_example.features.feature['labels'].int64_list.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qHrhuAZKfY2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN5BmuAEKjhg"
      },
      "source": [
        "The third attempt was re-run without the pip and import lines above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4tAQLXIwjxc"
      },
      "outputs": [],
      "source": [
        "\n",
        "vid_ids = []\n",
        "labels = []\n",
        "\n",
        "for example in tf.compat.v1.io.tf_record_iterator(frame_lvl_record):\n",
        "    tf_example = tf.train.Example.FromString(example)\n",
        "    vid_ids.append(tf_example.features.feature['id']\n",
        "                   .bytes_list.value[0].decode(encoding='UTF-8'))\n",
        "    labels.append(tf_example.features.feature['labels'].int64_list.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W-UGZO_wuCc",
        "outputId": "d6766523-6f33-4968-f236-ae562c74e8e4"
      },
      "outputs": [],
      "source": [
        "print('Number of videos in this tfrecord: ',len(vid_ids))\n",
        "print ('Number of labels in this tfrecord: ', len (labels))\n",
        "print('Picking a youtube video id:',vid_ids[15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tcoV-z7D-Nz"
      },
      "source": [
        "As described on the YouTube8M download page, for privacy reasons, the video id has been randomly generated and does not directly correspond to the actual YouTube video id. To convert the id into the actual YouTube video id, we follow link: http://data.yt8m.org/2/j/i/UL/UL00.js\n",
        "For more information, go here: https://www.kaggle.com/code/inversion/starter-kernel-yt8m-2019-sample-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ9jgeH3xzVT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from IPython.display import YouTubeVideo #without this line, YouTubeVideo will NOT work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "RViOexhxw4hT",
        "outputId": "79b52fda-cd48-44f7-bbc3-3bdf0621bfc2"
      },
      "outputs": [],
      "source": [
        "# With that video id, we can play the video\n",
        "YouTubeVideo('UzXQaOLQVCU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2DwVT0B_reK",
        "outputId": "12291d38-5ccb-4aa3-8b11-278c165735b5"
      },
      "outputs": [],
      "source": [
        "print(vid_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LwpvWsGHBGCS",
        "outputId": "176fde1f-c8c1-414b-d46f-d670d164a5a7"
      },
      "outputs": [],
      "source": [
        "vid_ids[15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKO4fIpUBZe-",
        "outputId": "edee60d5-913d-42ac-a2f0-b206445c0530"
      },
      "outputs": [],
      "source": [
        "labels[15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "YitaI5f4AKM3",
        "outputId": "6778189c-0658-4c11-bb82-d3d55f6ecfa4"
      },
      "outputs": [],
      "source": [
        " YouTubeVideo('jO00') #does NOT work because the original YouTube id was stripped off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX66UeUIAaLY",
        "outputId": "a24903dc-4235-4389-fff9-d874048e2fe9"
      },
      "outputs": [],
      "source": [
        " print(tf_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pX-asVhH8PV"
      },
      "source": [
        "To get the original id, go to http://data.yt8m.org/2/j/i/FF/FF00.js which was created by mimicking the format here: http://data.yt8m.org/2/j/i/UL/UL00.js"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "sCzehPXnEYnG",
        "outputId": "f570ae2c-7896-4496-ebda-fc512135850c"
      },
      "outputs": [],
      "source": [
        "YouTubeVideo(\"CuEjiRyQhvc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_WKjW4eK_Ld"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IRKZn3cLmvG"
      },
      "outputs": [],
      "source": [
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hEht2BSL67i"
      },
      "outputs": [],
      "source": [
        "# # due to execution time, we're only going to read the first video\n",
        "\n",
        "# feat_rgb = []\n",
        "# feat_audio = []\n",
        "\n",
        "# for example in tf.compat.v1.io.tf_record_iterator(frame_lvl_record):  #changed from tf.python_io.tf_record_iterator by Afia\n",
        "#     tf_seq_example = tf.train.SequenceExample.FromString(example)\n",
        "#     n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n",
        "#     sess = tf.InteractiveSession()\n",
        "#     rgb_frame = []\n",
        "#     audio_frame = []\n",
        "#     # iterate through frames\n",
        "#     for i in range(n_frames):\n",
        "#         rgb_frame.append(tf.cast(tf.decode_raw(\n",
        "#                 tf_seq_example.feature_lists.feature_list['rgb']\n",
        "#                   .feature[i].bytes_list.value[0],tf.uint8)\n",
        "#                        ,tf.float32).eval())\n",
        "#         audio_frame.append(tf.cast(tf.decode_raw(\n",
        "#                 tf_seq_example.feature_lists.feature_list['audio']\n",
        "#                   .feature[i].bytes_list.value[0],tf.uint8)\n",
        "#                        ,tf.float32).eval())\n",
        "\n",
        "\n",
        "#     sess.close()\n",
        "\n",
        "#     feat_audio.append(audio_frame)\n",
        "#     feat_rgb.append(rgb_frame)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKRegnhQOyE4"
      },
      "outputs": [],
      "source": [
        "#This line of code is needed to disable the \"eager execution\". More details on stackoverflow\n",
        "tf.compat.v1.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tezHTVpAK-rm"
      },
      "outputs": [],
      "source": [
        "# due to execution time, we're only going to read the first video\n",
        "\n",
        "feat_rgb = []\n",
        "feat_audio = []\n",
        "\n",
        "for example in tf.compat.v1.io.tf_record_iterator(frame_lvl_record):  #changed from tf.python_io.tf_record_iterator by Afia\n",
        "    tf_seq_example = tf.train.SequenceExample.FromString(example)\n",
        "    n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n",
        "    sess = tf.compat.v1.InteractiveSession()  #changed from tf.InteractiveSession()\n",
        "    rgb_frame = []\n",
        "    audio_frame = []\n",
        "    # iterate through frames\n",
        "    for i in range(n_frames):\n",
        "        rgb_frame.append(tf.cast(tf.io.decode_raw( #changed from tf.decode_raw by Afia\n",
        "                tf_seq_example.feature_lists.feature_list['rgb']\n",
        "                  .feature[i].bytes_list.value[0],tf.uint8)\n",
        "                      ,tf.float32).eval())\n",
        "        audio_frame.append(tf.cast(tf.io.decode_raw(\n",
        "                tf_seq_example.feature_lists.feature_list['audio']\n",
        "                  .feature[i].bytes_list.value[0],tf.uint8)\n",
        "                       ,tf.float32).eval())\n",
        "\n",
        "\n",
        "    sess.close()\n",
        "\n",
        "    feat_audio.append(audio_frame)\n",
        "    feat_rgb.append(rgb_frame)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAvDNFtmPiP_",
        "outputId": "a1e07fb9-2e7b-4349-edda-cab35a93daa1"
      },
      "outputs": [],
      "source": [
        "print('The first video has %d frames' %len(feat_rgb[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id3cCwvHWhum"
      },
      "source": [
        "Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts_EVkrCWzgU"
      },
      "outputs": [],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRkXFnoWW4-U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu7lIvO4WhFv"
      },
      "outputs": [],
      "source": [
        "vocabulary = pd.read_csv('/content/vocabulary.csv')\n",
        "vocabulary.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TechEx_Project_3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
