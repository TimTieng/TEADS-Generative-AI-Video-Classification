{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech Exellence Advanced Data Science - Generative AI Video Classification Project\n",
    "\n",
    "##### Project Authors: Tim Tieng, Afia Owusu-Forfie\n",
    "\n",
    "**Objective**: Develop a model to classify video content into categories such as sports, news, movies, etc., and enhance this classification by generating descriptive captions or summaries that provide additional context about the content. This can be particularly useful for content curation platforms, accessibility applications (e.g., providing descriptions for the hearing impaired), or educational tools where supplementary information enhances learning.\n",
    "\n",
    "**Data**: Public Dataset: k400-Dataset, which has a vast collection of labeled video data suitable for training video classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages for project\n",
    "\n",
    "# Standard Libraries\n",
    "import csv\n",
    "import cv2\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import utils\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Algorithms, Modeling and Data Pre-processing\n",
    "import feature_engine\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from scipy.stats import anderson, chi2_contingency\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,precision_score, roc_auc_score,recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Deep Learning\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import RandomFlip, RandomRotation, Rescaling, BatchNormalization, Conv2D, MaxPooling2D, Dense, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Model Optimization and Hyperparameter Tuning\n",
    "import hyperopt\n",
    "from hyperopt import STATUS_OK, Trials, fmin, tpe, hp\n",
    "import mlflow\n",
    "\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the Data\n",
    "filepath = '../data/K400/video_annotations.csv'\n",
    "raw_csv = pd.read_csv(filepath)\n",
    "k400_df = pd.DataFrame(raw_csv)\n",
    "\n",
    "k400_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial inspection of complete dataframe\n",
    "k400_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values/percentage of null values:\n",
    "\n",
    "k400_df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k400_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations \n",
    "\n",
    "1. No null values\n",
    "2. Over 240k Observations\n",
    "3. 6 Attributes of string/int datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for dup\n",
    "num_unique = k400_df.nunique()\n",
    "num_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "1. THere are 400 unique labels\n",
    "2. There are about 20K youtube_id with only about 850 videos\n",
    "3. Videos duration is only 10 seconds as annotated by the difference between time_start and time_end values\n",
    "\n",
    "**Next Steps**: to reduce the dimensionality, I need to create a function that will map a video file to a youtube id value in the video_annotations.csv file and create a new dataframe where we have a match. Data Cleaning required on the names of the video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_id_values = k400_df['youtube_id']\n",
    "print(f\"Total Youtube ID Values in Dataset: {youtube_id_values.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the amount of unique youtube_id\n",
    "number_unique_id = youtube_id_values.nunique()\n",
    "print(f\"Unique Youtube ID Values: {number_unique_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique values\n",
    "unique_youtube_id = youtube_id_values.unique()\n",
    "unique_youtube_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of video files we are working with\n",
    "def count_video_files(directory):\n",
    "    \"\"\"\n",
    "    Purpose - to get a video file count within a given directory\n",
    "    Arguments - directory variable that holds the filepath to a video directory\n",
    "    Returns - video_count of type integer\n",
    "    \n",
    "    \"\"\"\n",
    "    # Set the allowed video file extensions\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv']\n",
    "\n",
    "    # Initialize the count\n",
    "    video_count = 0\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file_path in glob.glob(os.path.join(directory, '*')):\n",
    "        # Check if the file has a video file extension\n",
    "        if os.path.isfile(file_path) and any(file_path.lower().endswith(ext) for ext in video_extensions):\n",
    "            video_count += 1\n",
    "\n",
    "    return video_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Funcationality and return video count\n",
    "\n",
    "# Provide the directory path to count video files\n",
    "directory_path = '../data/K400/videos'\n",
    "\n",
    "# Call the function to count video files\n",
    "num_videos = count_video_files(directory_path)\n",
    "print(f'Total number of video files: {num_videos} videos present')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Observations\n",
    "\n",
    "1. There seems to be a match with the youtube_id values in the video_annotations.csv file and the initial naming convention of the video files.\n",
    "2. The videofile names have a timestamp that highlights how the 10second video frame was captured. \n",
    "\n",
    "**Next Steps**: In order to load in local video data correctly, I need to perform regular expressions to rename the video files to exclude the timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis: Labels\n",
    "\n",
    "For this project, we will explore what labels are present in the Kinetic 400 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k400_labels = k400_df['label']\n",
    "k400_labels.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = k400_labels.unique()\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the top 50 labels in the k400 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each label\n",
    "label_counts = k400_labels.value_counts()\n",
    "\n",
    "# Select the top 10 labels\n",
    "top_20_labels = label_counts.head(20)\n",
    "\n",
    "# Create a countplot for the top 10 labels using Seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(y=top_20_labels.index, x=top_20_labels.values, palette='viridis')\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Top 20 Most Frequent Labels')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Label')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The most frequent label in the dataset is 'abseiling', which has a count close to 50. This suggests that videos of abseiling are very common in the dataset.\n",
    "\n",
    "The activities can be categorized into several groups:\n",
    "\n",
    "**Musical activities**: These include playing instruments like the violin, ukulele, trumpet, trombone, saxophone, recorder, piano, and organ. We can see that musical activities feature prominently in the dataset, indicating a possible focus on musical performance videos.\n",
    "\n",
    "**Sports and physical activities**: This group includes pole vault, playing tennis, squash or racquetball, and kickball, among others. These activities are likely to involve dynamic movement, which can be useful for training algorithms to recognize physical actions.\n",
    "\n",
    "**Recreational games**: Playing Monopoly is included, which is an indoor recreational game. This may suggest that tehre are other labels in the dataset that may represent indoor activities where movement may be minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing - Rename video file names for easier loading\n",
    "\n",
    "Purpose - This step is required in order to extract features and data from the raw video files from the Kinetic Dataset. This will help later on when we split our data to feed into our future model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_timestamp(filename):\n",
    "    \"\"\"\n",
    "    Purpose: to remove the timestampe suffix at the end of our local video files\n",
    "    Arguments: filename \n",
    "    Retunrs: Cleaned filename\n",
    "    \"\"\"\n",
    "    # Split the filename by underscores\n",
    "    parts = filename.rsplit('_')\n",
    "\n",
    "    # Filter out parts that are likely numbers\n",
    "    cleaned_parts = [part for part in parts if not part.isdigit()]\n",
    "\n",
    "    # Join the cleaned parts with underscores to form the new filename\n",
    "    cleaned_filename = '_'.join(cleaned_parts)\n",
    "\n",
    "    return cleaned_filename # Remove leading/trailing whitespaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(directory):\n",
    "    \"\"\"\n",
    "    Purpose: To rename all the local video files in our directory for future loading \n",
    "    Arguments: Filepath to the video directory\n",
    "    Returns: None\n",
    "\n",
    "    Other Functions: Calls the remove_timestamp()\n",
    "    \"\"\"\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file is a regular file (not a directory)\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            # Remove timestamp from the filename\n",
    "            new_filename = remove_timestamp(filename)\n",
    "            # Rename the file if the filename has changed\n",
    "            if new_filename != filename:\n",
    "                os.rename(os.path.join(directory, filename),\n",
    "                          os.path.join(directory, new_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "video_directory = \"../data/K400/videos\"\n",
    "\n",
    "rename_files(video_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_directory = '../data/K400/videos'\n",
    "\n",
    "# Iterate through each YouTube ID\n",
    "for youtube_id in youtube_id_values:\n",
    "    # Find the corresponding video file in the directory\n",
    "    for filename in os.listdir(video_directory):\n",
    "        if youtube_id in filename:\n",
    "            # Extract the file extension\n",
    "            file_extension = os.path.splitext(filename)[1]\n",
    "\n",
    "            # Construct the new file name without the timestamp\n",
    "            new_filename = youtube_id + file_extension\n",
    "\n",
    "            # Construct the full paths for old and new files\n",
    "            old_filepath = os.path.join(video_directory, filename)\n",
    "            new_filepath = os.path.join(video_directory, new_filename)\n",
    "\n",
    "            # Rename the file\n",
    "            os.rename(old_filepath, new_filepath)\n",
    "            print(f'Renamed {filename} to {new_filename}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Removed the start_time portion of the timestamp, but left the end_time timestamp in the video file name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_percent = .30\n",
    "\n",
    "# Split the k400_df into test and train df\n",
    "\n",
    "# Split the dataframe into train and test using pd.sample()\n",
    "test_df = k400_df.sample(frac=split_percent, random_state=42)\n",
    "train_df = k400_df.drop(test_df.index)\n",
    "\n",
    "# Reset the index of the new dataframes\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "train_df has 13934 Rows with 6 Attributes\n",
    "\n",
    "test_df has 5972 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing - Video Data\n",
    "\n",
    "To help with video preprocessing, I will create several functions that will help with a specific step in the process. \n",
    "\n",
    "**Crop_center_square()**: I begin by cropping each video frame to a centered square using my crop_center_square(frame) function to ensure uniformity in size and aspect ratio. \n",
    "\n",
    "**Load_video()**: I then load and preprocess the videos with my load_video(path) function, which resizes the frames to a consistent dimension and assembles them into a numpy array, capping the number at max_frames if needed. \n",
    "\n",
    "**Prepare_all_videos()**: Finally, I extract features from all the frames using a pre-trained InceptionV3 model in my prepare_all_videos(df, root_dir) function and create masks to accommodate sequences of varying lengths, which gives me a set of tensors ready to feed into my transformer model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    \"\"\"\n",
    "    Purpose: Crop a given frame of a video into a centered squared. This will create uniformity across all the video frames available.\n",
    "    Arguments:\n",
    "        - frame: this is a numpy array representinga  provided video frame. Dimensions of the numpy array should be at least 2-D\n",
    "    Returns: a numpy array of cropped square of the input frame\n",
    "    \"\"\"\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    \"\"\"\n",
    "    Purpose: Build a feature extractor model using InceptionV3 design.\n",
    "    Arguments: None\n",
    "    Returns: a Keras model that takes an image shape of our hyperparameters defined earlier as input and outputs a flattened feature vector\n",
    "\n",
    "\n",
    "    Notes: This function initializes the InceptionV3 model with pre-trained ImageNet weights,\n",
    "    excluding the top (final fully connected) layers. It modifies the network to use\n",
    "    average pooling at the end and sets the expected input shape for the images. The\n",
    "    output is a keras Model that takes an image input and outputs the corresponding\n",
    "    feature vector. This model can be used to extract features from frames of a video\n",
    "    for further analysis or processing.\n",
    "    \"\"\"\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"label\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    \"\"\"\n",
    "    Purpose: to prepare local video data and labels for training in a sequence model (Transformers)\n",
    "    Parameters:\n",
    "        1. df: a pandas dataframe containing the columns youtube_id and label (From the schema of K400 Dataset)\n",
    "        2. root_dir: The filepath to where the k400 datasets are stored locally\n",
    "    Returns: A tuple containing two elements:  The first element is another tuple with\n",
    "      two numpy arrays: frame_features of shape (num_samples, MAX_SEQ_LENGTH, NUM_FEATURES)\n",
    "      and frame_masks of shape (num_samples, MAX_SEQ_LENGTH), both of which are prepared\n",
    "      for the sequence model. The second element is a numpy array of processed labels.\n",
    "    \"\"\"\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"youtube_id\"].values.tolist()\n",
    "    labels = df[\"label\"].values\n",
    "    labels = keras.ops.convert_to_numpy(label_processor(labels[..., None]))\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(\n",
    "            shape=(\n",
    "                1,\n",
    "                MAX_SEQ_LENGTH,\n",
    "            ),\n",
    "            dtype=\"bool\",\n",
    "        )\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :], verbose=0,\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test functionality\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Explanation:\n",
    "\n",
    "**Frame Features (13934, 20, 2048)**:\n",
    "    1. 13934 represents the number of video samples in the training set.\n",
    "    2. 20 indicates the fixed number of frames that have been selected or sampled from each video, which is the maximum sequence length (MAX_SEQ_LENGTH). This is the temporal dimension representing time steps in the video.\n",
    "    3. 2048 is the number of features extracted from each frame by the feature extraction model (e.g., InceptionV3). These features are a compressed representation of the frame's visual content.\n",
    "\n",
    "**Frame Masks (13934, 20)**:\n",
    "    1. This array is two-dimensional.\n",
    "    2. 13934 corresponds to the number of video samples, aligning with the frame_features dimension.\n",
    "    3. 20 represents the same temporal sequence length, with each element being a boolean (True/False or 1/0). The mask indicates which time steps in the sequence are actual data and which are padding. A value of 1 might denote actual data (not masked), and a value of 0 might denote padding (masked).\n",
    "\n",
    "\n",
    "**Next Steps**:\n",
    "\n",
    "1. The frame features will bed into a custom video classification model using Keras with the goal of learning from the visual content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing Custom Transformer Model for Video Classification\n",
    "\n",
    "In this step, we will create a custom transformer model designed for video classification. To accomplish this, we will experiment with the architecture of the layers to design a function model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create callbacks for our custom model:\n",
    "\n",
    "For this model, we will include:\n",
    "\n",
    "1. Tensorboard callback for experiment tracking.\n",
    "2. EarlyStopping Callback to stop training when a val_loss metric has stopped improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the log directory for tensorboard logs for our tensorboard callback\n",
    "import datetime as dt\n",
    "\n",
    "log_dir = \"../Logs/fit/\" + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience= 3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.position_embeddings.build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        inputs = keras.ops.cast(inputs, self.compute_dtype)\n",
    "        length = keras.ops.shape(inputs)[1]\n",
    "        positions = keras.ops.arange(start=0, stop=length, step=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PositionalEmbedding Class Explained\n",
    "\n",
    "This code defines a custom layer in TensorFlow using the Keras API. The purpose of this layer is to add positional information to the input data, which is essential for models like Transformers that do not inherently process sequential data in order. This is further confirmed in the Keras documentation that self-attention layers that form the basic blocks of a Transformer are order-agnostic. Since videos are ordered sequences of frames, we need our Transformer model to take into account order information. We do this via positional encoding. We simply embed the positions of the frames present inside videos with an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=keras.activations.gelu),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "The above classes will be instantiated within our get_compiled_model() below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(shape):\n",
    "    \"\"\"\n",
    "    Purpose: Builds and compiles a Transformer-based model for video classification.\n",
    "\n",
    "    This function constructs a Keras model with a Transformer encoder structure, \n",
    "    including positional embeddings for sequence data, a Transformer encoder layer, \n",
    "    and classification layers. The model takes an input of a specified shape and \n",
    "    outputs class probabilities for video classification. The model is compiled with \n",
    "    the Adam optimizer, sparse categorical cross-entropy loss, and tracks accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    - shape (tuple): The shape of the input data, excluding the batch size. \n",
    "      This should match the dimensions of the video frames being fed into the model.\n",
    "\n",
    "    Returns:\n",
    "    - keras.Model: The compiled Keras model ready for training.\n",
    "    \"\"\"\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "    classes = len(label_processor.get_vocabulary())\n",
    "\n",
    "\n",
    "    # inputs = keras.Input(shape=shape)\n",
    "    inputs = keras.Input(shape=(20,2048))\n",
    "    # Debugging Code\n",
    "    print(\"Shape before Positional Embedding:\", inputs.shape)\n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    \"\"\"\n",
    "    Purpose: Execute the training experiment for the video classification model.\n",
    "\n",
    "    This function handles the workflow of training a video classification model.\n",
    "    It sets up a checkpointing system to save the best model weights, compiles the model,\n",
    "    fits the model on the training data with validation, and evaluates the model on test data.\n",
    "    It prints out the test accuracy and returns the trained model.\n",
    "\n",
    "    Returns:\n",
    "    - keras.Model: The trained Keras model after loading the best weights.\n",
    "\n",
    "    Notes:\n",
    "    - This function assumes that `get_compiled_model`, `train_data`, `train_labels`, \n",
    "      `test_data`, and `test_labels` are available in the scope and that `train_data` \n",
    "      and `test_data` are numpy arrays with the first dimension being the batch size \n",
    "      and the remaining dimensions matching the expected input shape of the model.\n",
    "    - `EPOCHS` is a constant that defines how many epochs the model will be trained for.\n",
    "    - The model's weights are saved to a temporary file path which is hardcoded in the function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filepath = \"/tmp/video_classifier.weights.h5\" #Todo Update to video path\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    model = get_compiled_model(train_data.shape[1:])\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        validation_split=0.15,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint, tensorboard_callback, earlystopping_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    _, accuracy = model.evaluate(test_data, test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters \n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute on dummy\n",
    "\n",
    "model = get_compiled_model((MAX_SEQ_LENGTH, NUM_FEATURES, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary Notes:\n",
    "\n",
    "**Input Layer**: The model accepts input of shape (None, 20, 2048), where \"None\" can be any batch size, \"20\" is the sequence length, and \"2048\" is the feature dimension per sequence element.\n",
    "\n",
    "**Positional Embedding Layer**: This layer adds positional information to the input, outputting the same shape (None, 20, 2048) and has 40,960 parameter to ensure the model is learning  unique embedding for each position.\n",
    "\n",
    "**Transformer Layer**: The Transformer encoder layer maintains the shape (None, 20, 2048) with no change in sequence length or feature dimensionality, and has a 16,812,036 parameters. This layer is used for complex transformation to capture interactions between different positions in the sequence.\n",
    "\n",
    "**Global Max Pooling Layer**: Following the Transformer layer, a GlobalMaxPooling1D layer condenses the information across the sequence length from (None, 20, 2048) to (None, 2048), taking the maximum value over the sequence for each feature.\n",
    "\n",
    "**Dropout Layer**: A dropout layer with no change in shape (None, 2048) is used to *prevent overfitting* during training by randomly setting a portion of input units to 0 at each update during training time.\n",
    "\n",
    "**Dense Layer**: The final output layer is a dense layer with 400 units and a softmax activation to allow the model to performing classification into 400 categories. It contains 819,680 parameters.\n",
    "\n",
    "**Total Parameters**: The model has a total of 17,672,596 parameters, all of which are trainable. This is a large model with significant capacity for learning complex patterns in the data.\n",
    "\n",
    "**No Non-trainable Parameters**: There are 0 non-trainable parameters, which means all the parameters in the model are being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TechEx_Project_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
