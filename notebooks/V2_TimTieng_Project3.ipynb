{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech Exellence Advanced Data Science - Generative AI Video Classification Project\n",
    "\n",
    "##### Project Authors: Tim Tieng, Afia Owusu-Forfie\n",
    "\n",
    "**Objective**: Develop a model to classify video content into categories such as sports, news, movies, etc., and enhance this classification by generating descriptive captions or summaries that provide additional context about the content. This can be particularly useful for content curation platforms, accessibility applications (e.g., providing descriptions for the hearing impaired), or educational tools where supplementary information enhances learning.\n",
    "\n",
    "**Data**: Public Dataset: k400-Dataset, which has a vast collection of labeled video data suitable for training video classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages for project\n",
    "\n",
    "# Standard Libraries\n",
    "import csv\n",
    "import cv2\n",
    "import datetime as dt\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sys import stderr\n",
    "import utils\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Algorithms, Modeling and Data Pre-processing\n",
    "import feature_engine\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from scipy.stats import anderson, chi2_contingency\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,precision_score, roc_auc_score,recall_score\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler, StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Deep Learning\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import RandomFlip, RandomRotation, Rescaling, BatchNormalization, Conv2D, MaxPooling2D, Dense, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense,GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Model Optimization and Hyperparameter Tuning\n",
    "import hyperopt\n",
    "from hyperopt import STATUS_OK, Trials, fmin, tpe, hp\n",
    "import mlflow\n",
    "\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the Data\n",
    "filepath = '../data/K400/video_annotations.csv'\n",
    "raw_csv = pd.read_csv(filepath)\n",
    "k400_df = pd.DataFrame(raw_csv)\n",
    "\n",
    "k400_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial inspection of complete dataframe\n",
    "k400_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values/percentage of null values:\n",
    "\n",
    "k400_df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k400_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations \n",
    "\n",
    "1. No null values\n",
    "2. Over 240k Observations\n",
    "3. 6 Attributes of string/int datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for dup\n",
    "num_unique = k400_df.nunique()\n",
    "num_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "1. THere are 400 unique labels\n",
    "2. There are about 20K youtube_id with only about 850 videos\n",
    "3. Videos duration is only 10 seconds as annotated by the difference between time_start and time_end values\n",
    "\n",
    "**Next Steps**: to reduce the dimensionality, I need to create a function that will map a video file to a youtube id value in the video_annotations.csv file and create a new dataframe where we have a match. Data Cleaning required on the names of the video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_id_values = k400_df['youtube_id']\n",
    "print(f\"Total Youtube ID Values in Dataset: {youtube_id_values.count()}\")\n",
    "print(youtube_id_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the amount of unique youtube_id\n",
    "number_unique_id = youtube_id_values.nunique()\n",
    "print(f\"Unique Youtube ID Values: {number_unique_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique values\n",
    "unique_youtube_id = youtube_id_values.unique()\n",
    "unique_youtube_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of video files we are working with\n",
    "def count_video_files(directory):\n",
    "    \"\"\"\n",
    "    Purpose - to get a video file count within a given directory\n",
    "    Arguments - directory variable that holds the filepath to a video directory\n",
    "    Returns - video_count of type integer\n",
    "    \n",
    "    \"\"\"\n",
    "    # Set the allowed video file extensions\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv']\n",
    "\n",
    "    # Initialize the count\n",
    "    video_count = 0\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file_path in glob.glob(os.path.join(directory, '*')):\n",
    "        # Check if the file has a video file extension\n",
    "        if os.path.isfile(file_path) and any(file_path.lower().endswith(ext) for ext in video_extensions):\n",
    "            video_count += 1\n",
    "\n",
    "    return video_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Funcationality and return video count\n",
    "\n",
    "# Provide the directory path to count video files\n",
    "directory_path = '../data/K400/videos'\n",
    "\n",
    "# Call the function to count video files\n",
    "num_videos = count_video_files(directory_path)\n",
    "print(f'Total number of video files: {num_videos} videos present')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Observations\n",
    "\n",
    "1. There seems to be a match with the youtube_id values in the video_annotations.csv file and the initial naming convention of the video files.\n",
    "2. The videofile names have a timestamp that highlights how the 10second video frame was captured. \n",
    "\n",
    "**Next Steps**: In order to load in local video data correctly, I need to perform regular expressions to rename the video files to exclude the timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis: Labels\n",
    "\n",
    "For this project, we will explore what labels are present in the Kinetic 400 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k400_labels = k400_df['label']\n",
    "k400_labels.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = k400_labels.unique()\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels_count = k400_labels.nunique()\n",
    "print(f'Number of Unique Video Labels: {unique_labels_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the top 50 labels in the k400 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each label\n",
    "label_counts = k400_labels.value_counts()\n",
    "\n",
    "# Select the top 10 labels\n",
    "top_20_labels = label_counts.head(20)\n",
    "\n",
    "# Create a countplot for the top 10 labels using Seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(y=top_20_labels.index, x=top_20_labels.values, palette='viridis')\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Top 20 Most Frequent Labels')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Label')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The most frequent label in the dataset is 'abseiling', which has a count close to 50. This suggests that videos of abseiling are very common in the dataset.\n",
    "\n",
    "The activities can be categorized into several groups:\n",
    "\n",
    "**Musical activities**: These include playing instruments like the violin, ukulele, trumpet, trombone, saxophone, recorder, piano, and organ. We can see that musical activities feature prominently in the dataset, indicating a possible focus on musical performance videos.\n",
    "\n",
    "**Sports and physical activities**: This group includes pole vault, playing tennis, squash or racquetball, and kickball, among others. These activities are likely to involve dynamic movement, which can be useful for training algorithms to recognize physical actions.\n",
    "\n",
    "**Recreational games**: Playing Monopoly is included, which is an indoor recreational game. This may suggest that tehre are other labels in the dataset that may represent indoor activities where movement may be minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing - Rename video file names for easier loading\n",
    "\n",
    "Purpose - This step is required in order to extract features and data from the raw video files from the Kinetic Dataset. This will help later on when we split our data to feed into our future model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_timestamp(filename):\n",
    "    \"\"\"\n",
    "    Purpose: to remove the timestampe suffix at the end of our local video files\n",
    "    Arguments: filename \n",
    "    Retunrs: Cleaned filename\n",
    "    \"\"\"\n",
    "    # Split the filename by underscores\n",
    "    parts = filename.rsplit('_')\n",
    "\n",
    "    # Filter out parts that are likely numbers\n",
    "    cleaned_parts = [part for part in parts if not part.isdigit()]\n",
    "\n",
    "    # Join the cleaned parts with underscores to form the new filename\n",
    "    cleaned_filename = '_'.join(cleaned_parts)\n",
    "\n",
    "    return cleaned_filename # Remove leading/trailing whitespaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(directory):\n",
    "    \"\"\"\n",
    "    Purpose: To rename all the local video files in our directory for future loading \n",
    "    Arguments: Filepath to the video directory\n",
    "    Returns: None\n",
    "\n",
    "    Other Functions: Calls the remove_timestamp()\n",
    "    \"\"\"\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file is a regular file (not a directory)\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            # Remove timestamp from the filename\n",
    "            new_filename = remove_timestamp(filename)\n",
    "            # Rename the file if the filename has changed\n",
    "            if new_filename != filename:\n",
    "                os.rename(os.path.join(directory, filename),\n",
    "                          os.path.join(directory, new_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "video_directory = \"../data/K400/videos\"\n",
    "\n",
    "rename_files(video_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_directory = '../data/K400/videos'\n",
    "\n",
    "# Iterate through each YouTube ID\n",
    "for youtube_id in youtube_id_values:\n",
    "    # Find the corresponding video file in the directory\n",
    "    for filename in os.listdir(video_directory):\n",
    "        if youtube_id in filename:\n",
    "            # Extract the file extension\n",
    "            file_extension = os.path.splitext(filename)[1]\n",
    "\n",
    "            # Construct the new file name without the timestamp\n",
    "            new_filename = youtube_id + file_extension\n",
    "\n",
    "            # Construct the full paths for old and new files\n",
    "            old_filepath = os.path.join(video_directory, filename)\n",
    "            new_filepath = os.path.join(video_directory, new_filename)\n",
    "\n",
    "            # Rename the file\n",
    "            os.rename(old_filepath, new_filepath)\n",
    "            print(f'Renamed {filename} to {new_filename}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(k400_df[\"label\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters \n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048\n",
    "# Debug\n",
    "NUM_CLASSES = len(label_processor.get_vocabulary())\n",
    "INPUT_SHAPE = (MAX_SEQ_LENGTH, NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "train_df has 13934 Rows with 6 Attributes\n",
    "\n",
    "test_df has 5972 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing - Video Data\n",
    "\n",
    "To help with video preprocessing, I will create several functions that will help with a specific step in the process. \n",
    "\n",
    "**Crop_center_square()**: I begin by cropping each video frame to a centered square using my crop_center_square(frame) function to ensure uniformity in size and aspect ratio. \n",
    "\n",
    "**Load_video()**: I then load and preprocess the videos with my load_video(path) function, which resizes the frames to a consistent dimension and assembles them into a numpy array, capping the number at max_frames if needed. \n",
    "\n",
    "**Prepare_all_videos()**: Finally, I extract features from all the frames using a pre-trained InceptionV3 model in my prepare_all_videos(df, root_dir) function and create masks to accommodate sequences of varying lengths, which gives me a set of tensors ready to feed into my transformer model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    \"\"\"\n",
    "    Purpose: Crop a given frame of a video into a centered squared. This will create uniformity across all the video frames available.\n",
    "    Arguments:\n",
    "        - frame: this is a numpy array representinga  provided video frame. Dimensions of the numpy array should be at least 2-D\n",
    "    Returns: a numpy array of cropped square of the input frame\n",
    "    \"\"\"\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video2(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"File does not exist: {path}\", file=stderr)\n",
    "        return np.array([])  # Return an empty array if file doesn't exist\n",
    "    \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Cannot open video: {path}\", file=stderr)\n",
    "        return np.array([])  # Return an empty array if video cannot be opened\n",
    "\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                if len(frames) == 0:\n",
    "                    print(f\"Failed to capture any frames from video: {path}\", file=stderr)\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame.astype('float32')  # Ensure frame is in float format before preprocessing\n",
    "            frame = frame[:, :, [2, 1, 0]]  # Convert from BGR to RGB\n",
    "            \n",
    "            # Apply model-specific preprocessing\n",
    "            frame = preprocess_input(frame)\n",
    "\n",
    "            frames.append(frame)\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    \"\"\"\n",
    "    Purpose: Build a feature extractor model using InceptionV3 design.\n",
    "    Arguments: None\n",
    "    Returns: a Keras model that takes an image shape of our hyperparameters defined earlier as input and outputs a flattened feature vector\n",
    "\n",
    "\n",
    "    Notes: This function initializes the InceptionV3 model with pre-trained ImageNet weights,\n",
    "    excluding the top (final fully connected) layers. It modifies the network to use\n",
    "    average pooling at the end and sets the expected input shape for the images. The\n",
    "    output is a keras Model that takes an image input and outputs the corresponding\n",
    "    feature vector. This model can be used to extract features from frames of a video\n",
    "    for further analysis or processing.\n",
    "    \"\"\"\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(k400_df[\"label\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    \"\"\"\n",
    "    Purpose: to prepare local video data and labels for training in a sequence model (Transformers)\n",
    "    Parameters:\n",
    "        1. df: a pandas dataframe containing the columns youtube_id and label (From the schema of K400 Dataset)\n",
    "        2. root_dir: The filepath to where the k400 datasets are stored locally\n",
    "    Returns: A tuple containing two elements:  The first element is another tuple with\n",
    "      two numpy arrays: frame_features of shape (num_samples, MAX_SEQ_LENGTH, NUM_FEATURES)\n",
    "      and frame_masks of shape (num_samples, MAX_SEQ_LENGTH), both of which are prepared\n",
    "      for the sequence model. The second element is a numpy array of processed labels.\n",
    "    \"\"\"\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"youtube_id\"].values.tolist()\n",
    "    labels = df[\"label\"].values\n",
    "    labels = keras.ops.convert_to_numpy(label_processor(labels[..., None]))\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(\n",
    "            shape=(\n",
    "                1,\n",
    "                MAX_SEQ_LENGTH,\n",
    "            ),\n",
    "            dtype=\"bool\",\n",
    "        )\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :], verbose=0,\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos2(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df['youtube_id'].values.tolist()  # Assuming these are IDs that need '.mp4' appended\n",
    "    labels = df['label'].values\n",
    "\n",
    "    MAX_SEQ_LENGTH = 10  # Example maximum sequence length\n",
    "    NUM_FEATURES = 2048  # Example number of features extracted by your feature extractor\n",
    "\n",
    "    frame_features = np.zeros((num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype='float32')\n",
    "    frame_masks = np.zeros((num_samples, MAX_SEQ_LENGTH), dtype=bool)\n",
    "\n",
    "    for idx, video_id in enumerate(video_paths):\n",
    "        video_path = os.path.join(root_dir, video_id + '.mp4')  # Ensure path includes '.mp4'\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"File does not exist: {video_path}\")  # Debugging output\n",
    "            continue  # Skip this iteration if file doesn't exist\n",
    "\n",
    "        frames = load_video2(video_path)  # Ensure your load_video2 can handle the full path with .mp4\n",
    "\n",
    "        if frames.size == 0:\n",
    "            print(f\"No frames loaded from video: {video_path}\")  # Additional debug info\n",
    "            continue\n",
    "\n",
    "        num_frames = frames.shape[0]\n",
    "        num_frames_used = min(MAX_SEQ_LENGTH, num_frames)\n",
    "\n",
    "        for j in range(num_frames_used):\n",
    "            frame = np.expand_dims(frames[j], axis=0)  # Adding batch dimension\n",
    "            features = feature_extractor.predict(frame)[0]  # Extract features\n",
    "            frame_features[idx, j, :] = features\n",
    "            frame_masks[idx, j] = 1  # Mark this frame as used\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_directory = '../data/K400/videos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on smaller batch of files\n",
    "test_path = r\"C:\\Users\\timot\\OneDrive\\Desktop\\TechEx_Project_3\\TEADS-Generative-AI-Video-Classification\\data\\K400\\Test Videos\"\n",
    "\n",
    "\n",
    "# This could be a list you either load or define. For example:\n",
    "youtube_id_values2 = ['0aAhM7nyEmM', '0BLpSquuZFM', '0TPxcKUNxbA']\n",
    "\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for file in os.listdir(test_path):\n",
    "    # Check if the file matches any YouTube ID in your list\n",
    "    for youtube_id in youtube_id_values2:\n",
    "        if file.startswith(youtube_id) and not file.endswith('.mp4'):\n",
    "            # Construct the full old file path\n",
    "            old_file_path = os.path.join(test_path, file)\n",
    "            # Add the .mp4 extension to the existing filename\n",
    "            new_file_path = os.path.join(test_path, file + '.mp4')\n",
    "            # Rename the file\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            print(f\"Renamed {file} to {file + '.mp4'}\")\n",
    "            break  # Stop checking other IDs once a match is found and renamed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on smalelr dataset\n",
    "k400_raw = pd.read_csv(\"../data/K400/test_video_annotations.csv\")\n",
    "k400_trunc = pd.DataFrame(k400_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on smaller set of videos\n",
    "train_data3, train_labels3 = prepare_all_videos2(k400_trunc, test_path)\n",
    "test_data3, test_labels3 = prepare_all_videos2(k400_trunc, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Frame features in train set: {train_data3[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data3[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data\n",
    "train_features3, train_masks3 = train_data3\n",
    "train_features3 = np.array(train_features3)\n",
    "train_masks3 = np.array(train_masks3)\n",
    "\n",
    "# Convert test data\n",
    "test_features3, test_masks3 = test_data3\n",
    "test_features3 = np.array(test_features3)\n",
    "test_masks3 = np.array(test_masks3)\n",
    "\n",
    "# Convert labels\n",
    "train_labels3 = np.array(train_labels3)\n",
    "test_labels3 = np.array(test_labels3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "print(f\"Train Features Shape: {train_features3.shape}, Type: {type(train_features3)}\")\n",
    "print(f\"Train Masks Shape: {train_masks3.shape}, Type: {type(train_masks3)}\")\n",
    "print(f\"Test Features Shape: {test_features3.shape}, Type: {type(test_features3)}\")\n",
    "print(f\"Test Masks Shape: {test_masks3.shape}, Type: {type(test_masks3)}\")\n",
    "print(f\"Train Labels Shape: {train_labels3.shape}, Type: {type(train_labels3)}\")\n",
    "print(f\"Test Labels Shape: {test_labels3.shape}, Type: {type(test_labels3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the values within the train_features3 object\n",
    "print(train_features3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Explanation:\n",
    "\n",
    "The output confirms that our test_labels has data that can be used in the transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Data\n",
    "\n",
    "Encoding categorical values is crucial in video classification models to ensure compatibility with machine learning algorithms, improve model performance, and prevent biases in predictions. \n",
    "By doing this, we are able to transform the data into a format that is consumable by the model. To  accomplish this, we will instatiate an object of the LabelEncoder(), then fit_transform() the data of train_labels3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder instance\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the training labels, then transform them to integers\n",
    "train_labels3 = encoder.fit_transform(train_labels3)\n",
    "\n",
    "# If you have test labels, transform them using the same encoder\n",
    "test_labels3 = encoder.transform(test_labels3)\n",
    "\n",
    "# Check the transformed labels and their data type\n",
    "print(train_labels3.dtype)\n",
    "print(train_labels3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________**\n",
    "\n",
    "**_____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing Custom Transformer Model for Video Classification\n",
    "\n",
    "In this step, we will create a custom transformer model designed for video classification. To accomplish this, we will experiment with the architecture of the layers to design a function model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create callbacks for our custom model:\n",
    "\n",
    "For this model, we will include:\n",
    "\n",
    "1. Tensorboard callback for experiment tracking.\n",
    "2. EarlyStopping Callback to stop training when a val_loss metric has stopped improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the log directory for tensorboard logs for our tensorboard callback\n",
    "\n",
    "\n",
    "log_dir = \"../Logs/fit/\" + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience= 3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.position_embeddings.build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        inputs = keras.ops.cast(inputs, self.compute_dtype)\n",
    "        length = keras.ops.shape(inputs)[1]\n",
    "        positions = keras.ops.arange(start=0, stop=length, step=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PositionalEmbedding Class Explained\n",
    "\n",
    "This code defines a custom layer in TensorFlow using the Keras API. The purpose of this layer is to add positional information to the input data, which is essential for models like Transformers that do not inherently process sequential data in order. This is further confirmed in the Keras documentation that self-attention layers that form the basic blocks of a Transformer are order-agnostic. Since videos are ordered sequences of frames, we need our Transformer model to take into account order information. We do this via positional encoding. We simply embed the positions of the frames present inside videos with an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=keras.activations.gelu),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Debug: Ensure mask is properly broadcasted for the attention mechanism\n",
    "        if mask is not None:\n",
    "            # Expand mask dimensions to [batch_size, 1, 1, sequence_length]\n",
    "            mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], tf.int32)\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_transformer_model(input_shape, num_classes):\n",
    "    # Unpack input_shape tuple\n",
    "    sequence_length, embed_dim = input_shape\n",
    "\n",
    "    # Define the inputs\n",
    "    input_features = Input(shape=input_shape, name=\"input_features\")\n",
    "    input_masks = Input(shape=(sequence_length,), name=\"input_masks\", dtype='bool') \n",
    "\n",
    "    # Assuming PositionalEmbedding and TransformerEncoder are defined elsewhere\n",
    "    x = PositionalEmbedding(sequence_length, embed_dim, name=\"frame_position_embedding\")(input_features)\n",
    "    x = TransformerEncoder(embed_dim, 4 * embed_dim, 1, name=\"transformer_layer\")(x, mask=input_masks)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[input_features, input_masks], outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_features, train_masks, train_labels, test_features, test_masks, test_labels, num_classes):\n",
    "    model = get_compiled_transformer_model(train_features.shape[1:], num_classes)  # Ensure this expects two inputs\n",
    "\n",
    "    filepath = \"video_classifier.weights.h5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "    earlystopping_callback = keras.callbacks.EarlyStopping(monitor='accuracy', patience=3, verbose=1)\n",
    "\n",
    "    # Ensure you pass both train_features and train_masks\n",
    "    history = model.fit(\n",
    "        [train_features, train_masks],  # Pass as a list if your model expects two inputs\n",
    "        train_labels,\n",
    "        validation_split=0.15,\n",
    "        epochs=10,\n",
    "        callbacks=[checkpoint, tensorboard_callback, earlystopping_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    _, accuracy = model.evaluate([test_features, test_masks], test_labels)  # Similarly, adjust here as needed\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters \n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048\n",
    "# Debug\n",
    "NUM_CLASSES = len(label_processor.get_vocabulary())\n",
    "INPUT_SHAPE = (MAX_SEQ_LENGTH, NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = get_compiled_transformer_model(INPUT_SHAPE, NUM_CLASSES)\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary Notes:\n",
    "\n",
    "**Input Layer**: The model accepts input of shape (None, 20, 2048), where \"None\" can be any batch size, \"20\" is the sequence length, and \"2048\" is the feature dimension per sequence element.\n",
    "\n",
    "**Positional Embedding Layer**: This layer adds positional information to the input, outputting the same shape (None, 20, 2048) and has 40,960 parameter to ensure the model is learning  unique embedding for each position.\n",
    "\n",
    "**Transformer Layer**: The Transformer encoder layer maintains the shape (None, 20, 2048) with no change in sequence length or feature dimensionality, and has a 16,812,036 parameters. This layer is used for complex transformation to capture interactions between different positions in the sequence.\n",
    "\n",
    "**Global Max Pooling Layer**: Following the Transformer layer, a GlobalMaxPooling1D layer condenses the information across the sequence length from (None, 20, 2048) to (None, 2048), taking the maximum value over the sequence for each feature.\n",
    "\n",
    "**Dropout Layer**: A dropout layer with no change in shape (None, 2048) is used to *prevent overfitting* during training by randomly setting a portion of input units to 0 at each update during training time.\n",
    "\n",
    "**Dense Layer**: The final output layer is a dense layer with 400 units and a softmax activation to allow the model to performing classification into 400 categories. It contains 819,680 parameters.\n",
    "\n",
    "**Total Parameters**: The model has a total of 17,672,596 parameters, all of which are trainable. This is a large model with significant capacity for learning complex patterns in the data.\n",
    "\n",
    "**No Non-trainable Parameters**: There are 0 non-trainable parameters, which means all the parameters in the model are being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = run_experiment(\n",
    "    train_features=train_features3,\n",
    "    train_masks=train_masks3,\n",
    "    train_labels=train_labels3,\n",
    "    test_features=test_features3,\n",
    "    test_masks=test_masks3,\n",
    "    test_labels=test_labels3,\n",
    "    num_classes=len(np.unique(train_labels3))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test_Model Resilts\n",
    "\n",
    "Epoch 1/5\n",
    "1/1     accuracy: 0.0000e+00 - loss: 5.8065\n",
    "Epoch 1: val_loss improved from inf to 0.00147, saving model to video_classifier.weights.h5\n",
    "1/1     accuracy: 0.0000e+00 - loss: 5.8065 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
    "\n",
    "Epoch 2/5\n",
    "1/1     accuracy: 1.0000 - loss: 0.0000e+00\n",
    "Epoch 2: val_loss did not improve from 0.00147\n",
    "1/1     accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0123\n",
    "\n",
    "Epoch 3/5\n",
    "1/1     accuracy: 1.0000 - loss: 0.0000e+00\n",
    "Epoch 3: val_loss did not improve from 0.00147\n",
    "1/1     accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0594\n",
    "\n",
    "Epoch 4/5\n",
    "1/1     accuracy: 1.0000 - loss: 0.0000e+00\n",
    "Epoch 4: val_loss did not improve from 0.00147\n",
    "1/1     accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.1787\n",
    "\n",
    "Epoch 4: early stopping\n",
    "1/1     accuracy: 1.0000 - loss: 4.8856e-04\n",
    "\n",
    "Test accuracy: 100.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Transformer Model on 3 video files\n",
    "\n",
    "The purpose behind running our model on 3 video files only is to save time, computational power, and more time to tweak our Transformers architecture if needed. \n",
    "\n",
    "THe output above suggests that on 3 videos, the validation accuracy of the Transfomer model is 100%. This is good sign moving forward. Next step is to perform this model on all 1300 video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model on all video data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating updated video_annotations.csv file that holds info for only the 50 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all video files again\n",
    "video_directory2 = r'C:\\Users\\timot\\OneDrive\\Desktop\\TechEx_Project_3\\TEADS-Generative-AI-Video-Classification\\data\\K400\\videos'\n",
    "video_ids = [f[:-4] for f in os.listdir(video_directory) if f.endswith('.mp4')]\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_annotations = k400_df[k400_df['youtube_id'].isin(video_ids)]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "output_path = '../data/K400/filtered_video_annotations.csv'  # Adjust the path to the desired location\n",
    "filtered_annotations.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k400_v3 = pd.read_csv(\"../data/K400/filtered_video_annotations.csv\")\n",
    "k400_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_id_values3 = k400_v3['youtube_id']\n",
    "youtube_id_values3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each file in the directory\n",
    "for file in os.listdir(video_directory2):\n",
    "    # Check if the file matches any YouTube ID in your list\n",
    "    for youtube_id in youtube_id_values3:\n",
    "        if file.startswith(youtube_id) and not file.endswith('.mp4'):\n",
    "            # Construct the full old file path\n",
    "            old_file_path = os.path.join(video_directory2, file)\n",
    "            # Add the .mp4 extension to the existing filename\n",
    "            new_file_path = os.path.join(video_directory2, file + '.mp4')\n",
    "            # Rename the file\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            print(f\"Renamed {file} to {file + '.mp4'}\")\n",
    "            break  # Stop checking other IDs once a match is found and renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function on 50 video files\n",
    "train_data4, train_labels4 = prepare_all_videos2(k400_v3, video_directory2)\n",
    "test_data4, test_labels4 = prepare_all_videos2(k400_v3, video_directory2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data is present\n",
    "print(f\"Frame features in train set: {train_data4[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data4[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data\n",
    "train_features4, train_masks4 = train_data4\n",
    "train_features4 = np.array(train_features4)\n",
    "train_masks4 = np.array(train_masks4)\n",
    "\n",
    "# Convert test data\n",
    "test_features4, test_masks4 = test_data4\n",
    "test_features4 = np.array(test_features4)\n",
    "test_masks4 = np.array(test_masks4)\n",
    "\n",
    "# Convert labels\n",
    "train_labels4 = np.array(train_labels4)\n",
    "test_labels4 = np.array(test_labels4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding again\n",
    "\n",
    "# Create an encoder instance\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the training labels, then transform them to integers\n",
    "train_labels4 = encoder.fit_transform(train_labels4)\n",
    "\n",
    "# If you have test labels, transform them using the same encoder\n",
    "test_labels4 = encoder.transform(test_labels4)\n",
    "\n",
    "# Check the transformed labels and their data type\n",
    "print(train_labels4.dtype)\n",
    "print(train_labels4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify datatypes are numpy array for model consumption\n",
    "print(f\"Train Features Shape: {train_features4.shape}, Type: {type(train_features4)}\")\n",
    "print(f\"Train Masks Shape: {train_masks4.shape}, Type: {type(train_masks4)}\")\n",
    "print(f\"Test Features Shape: {test_features4.shape}, Type: {type(test_features4)}\")\n",
    "print(f\"Test Masks Shape: {test_masks4.shape}, Type: {type(test_masks4)}\")\n",
    "print(f\"Train Labels Shape: {train_labels4.shape}, Type: {type(train_labels4)}\")\n",
    "print(f\"Test Labels Shape: {test_labels4.shape}, Type: {type(test_labels4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify train_feature data\n",
    "print(train_features4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on 50 videos\n",
    "updated_model = run_experiment(\n",
    "    train_features=train_features4,\n",
    "    train_masks=train_masks4,\n",
    "    train_labels=train_labels4,\n",
    "    test_features=test_features4,\n",
    "    test_masks=test_masks4,\n",
    "    test_labels=test_labels4,\n",
    "    num_classes=len(np.unique(train_labels4))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated Model Output\n",
    "Epoch 1/10\n",
    "2/2  0s 473ms/step - accuracy: 0.0275 - loss: 6.5068\n",
    "Epoch 1: val_loss improved from inf to 11.23646, saving model to video_classifier.weights.h5\n",
    "2/2  9s 2s/step - accuracy: 0.0263 - loss: 6.8050 - val_accuracy: 0.0000e+00 - val_loss: 11.2365\n",
    "\n",
    "Epoch 2/10\n",
    "2/2  0s 389ms/step - accuracy: 0.2478 - loss: 5.2465\n",
    "Epoch 2: val_loss did not improve from 11.23646\n",
    "2/2  2s 518ms/step - accuracy: 0.2366 - loss: 5.4148 - val_accuracy: 0.0000e+00 - val_loss: 13.1163\n",
    "\n",
    "Epoch 3/10\n",
    "2/2  0s 361ms/step - accuracy: 0.1890 - loss: 7.6730\n",
    "Epoch 3: val_loss did not improve from 11.23646\n",
    "2/2  1s 445ms/step - accuracy: 0.1895 - loss: 7.5889 - val_accuracy: 0.0000e+00 - val_loss: 13.6285\n",
    "\n",
    "Epoch 4/10\n",
    "2/2  0s 521ms/step - accuracy: 0.3817 - loss: 5.3065\n",
    "Epoch 4: val_loss did not improve from 11.23646\n",
    "2/2  2s 617ms/step - accuracy: 0.3735 - loss: 5.3583 - val_accuracy: 0.0000e+00 - val_loss: 12.7680\n",
    "\n",
    "Epoch 5/10\n",
    "2/2  0s 411ms/step - accuracy: 0.4807 - loss: 2.8781\n",
    "Epoch 5: val_loss did not improve from 11.23646\n",
    "2/2  1s 506ms/step - accuracy: 0.4950 - loss: 2.7933 - val_accuracy: 0.0000e+00 - val_loss: 13.7134\n",
    "\n",
    "Epoch 6/10\n",
    "2/2  0s 370ms/step - accuracy: 0.8698 - loss: 1.3318\n",
    "Epoch 6: val_loss did not improve from 11.23646\n",
    "2/2  1s 459ms/step - accuracy: 0.8576 - loss: 1.3214 - val_accuracy: 0.0000e+00 - val_loss: 12.7168\n",
    "\n",
    "Epoch 7/10\n",
    "...\n",
    "2/2  2s 725ms/step - accuracy: 0.9315 - loss: 0.5047 - val_accuracy: 0.0000e+00 - val_loss: 12.7179\n",
    "\n",
    "Epoch 10: early stopping\n",
    "2/2  0s 146ms/step - accuracy: 0.3279 - loss: 5.1113\n",
    "\n",
    "**Test accuracy: 32.0%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/5\n",
    "2/2  0s 2s/step - accuracy: 0.0275 - loss: 6.4632  \n",
    "Epoch 1: val_loss improved from inf to 10.43713, saving model to video_classifier.weights.h5\n",
    "2/2  23s 6s/step - accuracy: 0.0263 - loss: 6.8820 - val_accuracy: 0.0000e+00 - val_loss: 10.4371\n",
    "Epoch 2/5\n",
    "2/2  0s 1s/step - accuracy: 0.3854 - loss: 5.1984 \n",
    "Epoch 2: val_loss did not improve from 10.43713\n",
    "2/2  11s 2s/step - accuracy: 0.3681 - loss: 5.2190 - val_accuracy: 0.0000e+00 - val_loss: 14.0470\n",
    "Epoch 3/5\n",
    "2/2  0s 566ms/step - accuracy: 0.3504 - loss: 6.8348\n",
    "Epoch 3: val_loss did not improve from 10.43713\n",
    "2/2  2s 706ms/step - accuracy: 0.3527 - loss: 6.7026 - val_accuracy: 0.0000e+00 - val_loss: 18.2162\n",
    "Epoch 4/5\n",
    "2/2  0s 433ms/step - accuracy: 0.5082 - loss: 4.3170\n",
    "Epoch 4: val_loss did not improve from 10.43713\n",
    "2/2  2s 571ms/step - accuracy: 0.5213 - loss: 4.1773 - val_accuracy: 0.0000e+00 - val_loss: 13.7990\n",
    "Epoch 4: early stopping\n",
    "2/2  0s 139ms/step - accuracy: 0.2567 - loss: 5.5129\n",
    "\n",
    "**Test accuracy: 26.0%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/10\n",
    "2/2  0s 407ms/step - accuracy: 0.0000e+00 - loss: 7.4225\n",
    "\n",
    "Epoch 1: val_loss improved from inf to 12.29370, saving model to video_classifier.weights.h5\n",
    "2/2  8s 2s/step - accuracy: 0.0000e+00 - loss: 7.8232 - val_accuracy: 0.0000e+00 - val_loss: 12.2937\n",
    "Epoch 2/10\n",
    "2/2  0s 397ms/step - accuracy: 0.4129 - loss: 4.4882\n",
    "Epoch 2: val_loss did not improve from 12.29370\n",
    "2/2  2s 498ms/step - accuracy: 0.3943 - loss: 4.8033 - val_accuracy: 0.0000e+00 - val_loss: 15.4482\n",
    "Epoch 3/10\n",
    "2/2  0s 400ms/step - accuracy: 0.0945 - loss: 7.3664\n",
    "Epoch 3: val_loss did not improve from 12.29370\n",
    "2/2  2s 525ms/step - accuracy: 0.0947 - loss: 7.4326 - val_accuracy: 0.0000e+00 - val_loss: 14.6893\n",
    "Epoch 4/10\n",
    "2/2  0s 402ms/step - accuracy: 0.3899 - loss: 5.1638\n",
    "Epoch 4: val_loss did not improve from 12.29370\n",
    "2/2  1s 502ms/step - accuracy: 0.3948 - loss: 5.1515 - val_accuracy: 0.0000e+00 - val_loss: 15.1793\n",
    "Epoch 5/10\n",
    "2/2  0s 369ms/step - accuracy: 0.4606 - loss: 3.8587\n",
    "Epoch 5: val_loss did not improve from 12.29370\n",
    "2/2  1s 453ms/step - accuracy: 0.4578 - loss: 3.8979 - val_accuracy: 0.0000e+00 - val_loss: 14.1787\n",
    "Epoch 6/10\n",
    "2/2  0s 385ms/step - accuracy: 0.7202 - loss: 2.1544\n",
    "Epoch 6: val_loss did not improve from 12.29370\n",
    "2/2  1s 489ms/step - accuracy: 0.7103 - loss: 2.3151 - val_accuracy: 0.0000e+00 - val_loss: 13.4490\n",
    "...\n",
    "Epoch 10: val_loss did not improve from 12.29370\n",
    "2/2  1s 470ms/step - accuracy: 0.9157 - loss: 0.2378 - val_accuracy: 0.0000e+00 - val_loss: 14.9661\n",
    "2/2  0s 121ms/step - accuracy: 0.4096 - loss: 5.2742\n",
    "\n",
    "\n",
    "**Test accuracy: 38.0%**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TechEx_Project_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
