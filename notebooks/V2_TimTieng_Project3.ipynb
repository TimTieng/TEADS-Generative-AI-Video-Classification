{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages for project\n",
    "\n",
    "# Standard Libraries\n",
    "import csv\n",
    "import cv2\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import utils\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Algorithms, Modeling and Data Pre-processing\n",
    "import feature_engine\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from scipy.stats import anderson, chi2_contingency\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,precision_score, roc_auc_score,recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Deep Learning\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import RandomFlip, RandomRotation, Rescaling, BatchNormalization, Conv2D, MaxPooling2D, Dense, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Model Optimization and Hyperparameter Tuning\n",
    "import hyperopt\n",
    "from hyperopt import STATUS_OK, Trials, fmin, tpe, hp\n",
    "import mlflow\n",
    "\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the Data\n",
    "filepath = '../data/K400/video_annotations.csv'\n",
    "raw_csv = pd.read_csv(filepath)\n",
    "k400_df = pd.DataFrame(raw_csv)\n",
    "\n",
    "k400_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial inspection of complete dataframe\n",
    "k400_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values/percentage of null values:\n",
    "\n",
    "k400_df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k400_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations \n",
    "\n",
    "1. No null values\n",
    "2. Over 240k Observations\n",
    "3. 6 Attributes of string/int datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for dup\n",
    "num_unique = k400_df.nunique()\n",
    "num_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "1. THere are 400 unique labels\n",
    "2. There are about 20K youtube_id with only about 850 videos\n",
    "3. Videos duration is only 10 seconds as annotated by the difference between time_start and time_end values\n",
    "\n",
    "**Next Steps**: to reduce the dimensionality, I need to create a function that will map a video file to a youtube id value in the video_annotations.csv file and create a new dataframe where we have a match. Data Cleaning required on the names of the video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_id_values = k400_df['youtube_id']\n",
    "print(f\"Total Youtube ID Values in Dataset: {youtube_id_values.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the amount of unique youtube_id\n",
    "number_unique_id = youtube_id_values.nunique()\n",
    "print(f\"Unique Youtube ID Values: {number_unique_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique values\n",
    "unique_youtube_id = youtube_id_values.unique()\n",
    "unique_youtube_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of video files we are working with\n",
    "def count_video_files(directory):\n",
    "    \"\"\"\n",
    "    Purpose - to get a video file count within a given directory\n",
    "    Arguments - directory variable that holds the filepath to a video directory\n",
    "    Returns - video_count of type integer\n",
    "    \n",
    "    \"\"\"\n",
    "    # Set the allowed video file extensions\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv']\n",
    "\n",
    "    # Initialize the count\n",
    "    video_count = 0\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file_path in glob.glob(os.path.join(directory, '*')):\n",
    "        # Check if the file has a video file extension\n",
    "        if os.path.isfile(file_path) and any(file_path.lower().endswith(ext) for ext in video_extensions):\n",
    "            video_count += 1\n",
    "\n",
    "    return video_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Funcationality and return video count\n",
    "\n",
    "# Provide the directory path to count video files\n",
    "directory_path = '../data/K400/videos'\n",
    "\n",
    "# Call the function to count video files\n",
    "num_videos = count_video_files(directory_path)\n",
    "print(f'Total number of video files: {num_videos} videos present')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Observations\n",
    "\n",
    "1. There seems to be a match with the youtube_id values in the video_annotations.csv file and the initial naming convention of the video files.\n",
    "2. The videofile names have a timestamp that highlights how the 10second video frame was captured. \n",
    "\n",
    "**Next Steps**: In order to load in local video data correctly, I need to perform regular expressions to rename the video files to exclude the timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_timestamp(filename):\n",
    "    \"\"\"\n",
    "    Purpose: to remove the timestampe suffix at the end of our local video files\n",
    "    Arguments: filename \n",
    "    Retunrs: Cleaned filename\n",
    "    \"\"\"\n",
    "    # Split the filename by underscores\n",
    "    parts = filename.rsplit('_')\n",
    "\n",
    "    # Filter out parts that are likely numbers\n",
    "    cleaned_parts = [part for part in parts if not part.isdigit()]\n",
    "\n",
    "    # Join the cleaned parts with underscores to form the new filename\n",
    "    cleaned_filename = '_'.join(cleaned_parts)\n",
    "\n",
    "    return cleaned_filename # Remove leading/trailing whitespaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(directory):\n",
    "    \"\"\"\n",
    "    Purpose: To rename all the local video files in our directory for future loading \n",
    "    Arguments: Filepath to the video directory\n",
    "    Returns: None\n",
    "\n",
    "    Other Functions: Calls the remove_timestamp()\n",
    "    \"\"\"\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file is a regular file (not a directory)\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            # Remove timestamp from the filename\n",
    "            new_filename = remove_timestamp(filename)\n",
    "            # Rename the file if the filename has changed\n",
    "            if new_filename != filename:\n",
    "                os.rename(os.path.join(directory, filename),\n",
    "                          os.path.join(directory, new_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "video_directory = \"../data/K400/videos\"\n",
    "\n",
    "rename_files(video_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_directory = '../data/K400/videos'\n",
    "\n",
    "# Iterate through each YouTube ID\n",
    "for youtube_id in youtube_id_values:\n",
    "    # Find the corresponding video file in the directory\n",
    "    for filename in os.listdir(video_directory):\n",
    "        if youtube_id in filename:\n",
    "            # Extract the file extension\n",
    "            file_extension = os.path.splitext(filename)[1]\n",
    "\n",
    "            # Construct the new file name without the timestamp\n",
    "            new_filename = youtube_id + file_extension\n",
    "\n",
    "            # Construct the full paths for old and new files\n",
    "            old_filepath = os.path.join(video_directory, filename)\n",
    "            new_filepath = os.path.join(video_directory, new_filename)\n",
    "\n",
    "            # Rename the file\n",
    "            os.rename(old_filepath, new_filepath)\n",
    "            print(f'Renamed {filename} to {new_filename}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Removed the start_time portion of the timestamp, but left the end_time timestamp in the video file name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_percent = .30\n",
    "\n",
    "# Split the k400_df into test and train df\n",
    "\n",
    "# Split the dataframe into train and test using pd.sample()\n",
    "test_df = k400_df.sample(frac=split_percent, random_state=42)\n",
    "train_df = k400_df.drop(test_df.index)\n",
    "\n",
    "# Reset the index of the new dataframes\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "train_df has 13934 Rows with 6 Attributes\n",
    "\n",
    "test_df has 5972 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"label\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"youtube_id\"].values.tolist()\n",
    "    labels = df[\"label\"].values\n",
    "    labels = keras.ops.convert_to_numpy(label_processor(labels[..., None]))\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(\n",
    "            shape=(\n",
    "                1,\n",
    "                MAX_SEQ_LENGTH,\n",
    "            ),\n",
    "            dtype=\"bool\",\n",
    "        )\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :], verbose=0,\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test functionality\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TechEx_Project_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
